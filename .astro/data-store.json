[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.10","content-config-digest","cf802c852efddf21","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://contextself.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","journal",["Map",11,12,43,44,62,63,139,140,201,202,238,239,279,280],"on-observing-without-judging",{"id":11,"data":13,"body":17,"filePath":18,"digest":19,"rendered":20,"legacyId":42},{"title":14,"date":15,"description":16},"On Observing Without Judging",["Date","2025-01-17T00:00:00.000Z"],"What can we actually know about human nature from signals?","I've been thinking about what it means to understand human behavior through data—photos, videos, signals extracted by machines. The technology exists to detect faces, estimate expressions, track movement. But the harder question is: what do these signals actually tell us?\n\nThere's a tendency to overreach. A model outputs \"angry: 0.73\" and we conclude the person is angry. But that number is a statistical pattern matching exercise, not a window into someone's mind. The face might be squinting from sunlight. The expression might be cultural. The moment might be unrepresentative.\n\n## What we can observe\n\n- Geometric facts: where faces are, how bodies are positioned, who appears near whom\n- Statistical tendencies: over time, signals cluster around certain values\n- Structural patterns: who co-occurs with whom, who seems central in a group\n\n## What we cannot observe\n\n- Why someone feels what they feel\n- What they're thinking\n- Their intent, their character, their truth\n\nThe gap between these two lists is vast, and humility lives in that gap.\n\n## Equilibrium as epistemology\n\nThere's something appealing about the concept of equilibrium here. Instead of trusting any single frame—any single moment of observation—we ask: what state do the signals stabilize toward? And if they don't stabilize, that's information too. It means the data is ambiguous, conflicting, insufficient.\n\nNon-convergence isn't failure. It's honesty.\n\n## The ethical line\n\nIf we build systems that observe human behavior, the constraints should be hard:\n\n- No claims about identity unless explicitly consented\n- No pretending probabilistic signals are ground truth\n- Refusal built into the system—it must decline to interpret when it cannot do so responsibly\n- Aggregate over individual—prefer patterns over profiles\n\nThe goal isn't to know people. It's to notice patterns in signals, with transparency about what those patterns are and aren't.\n\nWater doesn't claim to understand the stone it flows around. It just flows.\n\n---\n\n*These thoughts eventually shaped [taocore-human](https://github.com/daidaitaotao/taocore-human)—an attempt to build these constraints into code. Whether the implementation honors the philosophy remains to be seen.*","src/content/journal/on-observing-without-judging.md","a30c3066f34a8270",{"html":21,"metadata":22},"\u003Cp>I’ve been thinking about what it means to understand human behavior through data—photos, videos, signals extracted by machines. The technology exists to detect faces, estimate expressions, track movement. But the harder question is: what do these signals actually tell us?\u003C/p>\n\u003Cp>There’s a tendency to overreach. A model outputs “angry: 0.73” and we conclude the person is angry. But that number is a statistical pattern matching exercise, not a window into someone’s mind. The face might be squinting from sunlight. The expression might be cultural. The moment might be unrepresentative.\u003C/p>\n\u003Ch2 id=\"what-we-can-observe\">What we can observe\u003C/h2>\n\u003Cul>\n\u003Cli>Geometric facts: where faces are, how bodies are positioned, who appears near whom\u003C/li>\n\u003Cli>Statistical tendencies: over time, signals cluster around certain values\u003C/li>\n\u003Cli>Structural patterns: who co-occurs with whom, who seems central in a group\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"what-we-cannot-observe\">What we cannot observe\u003C/h2>\n\u003Cul>\n\u003Cli>Why someone feels what they feel\u003C/li>\n\u003Cli>What they’re thinking\u003C/li>\n\u003Cli>Their intent, their character, their truth\u003C/li>\n\u003C/ul>\n\u003Cp>The gap between these two lists is vast, and humility lives in that gap.\u003C/p>\n\u003Ch2 id=\"equilibrium-as-epistemology\">Equilibrium as epistemology\u003C/h2>\n\u003Cp>There’s something appealing about the concept of equilibrium here. Instead of trusting any single frame—any single moment of observation—we ask: what state do the signals stabilize toward? And if they don’t stabilize, that’s information too. It means the data is ambiguous, conflicting, insufficient.\u003C/p>\n\u003Cp>Non-convergence isn’t failure. It’s honesty.\u003C/p>\n\u003Ch2 id=\"the-ethical-line\">The ethical line\u003C/h2>\n\u003Cp>If we build systems that observe human behavior, the constraints should be hard:\u003C/p>\n\u003Cul>\n\u003Cli>No claims about identity unless explicitly consented\u003C/li>\n\u003Cli>No pretending probabilistic signals are ground truth\u003C/li>\n\u003Cli>Refusal built into the system—it must decline to interpret when it cannot do so responsibly\u003C/li>\n\u003Cli>Aggregate over individual—prefer patterns over profiles\u003C/li>\n\u003C/ul>\n\u003Cp>The goal isn’t to know people. It’s to notice patterns in signals, with transparency about what those patterns are and aren’t.\u003C/p>\n\u003Cp>Water doesn’t claim to understand the stone it flows around. It just flows.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>These thoughts eventually shaped \u003Ca href=\"https://github.com/daidaitaotao/taocore-human\">taocore-human\u003C/a>—an attempt to build these constraints into code. Whether the implementation honors the philosophy remains to be seen.\u003C/em>\u003C/p>",{"headings":23,"localImagePaths":37,"remoteImagePaths":38,"frontmatter":39,"imagePaths":41},[24,28,31,34],{"depth":25,"slug":26,"text":27},2,"what-we-can-observe","What we can observe",{"depth":25,"slug":29,"text":30},"what-we-cannot-observe","What we cannot observe",{"depth":25,"slug":32,"text":33},"equilibrium-as-epistemology","Equilibrium as epistemology",{"depth":25,"slug":35,"text":36},"the-ethical-line","The ethical line",[],[],{"title":14,"date":40,"description":16},["Date","2025-01-17T00:00:00.000Z"],[],"on-observing-without-judging.md","on-starting",{"id":43,"data":45,"body":49,"filePath":50,"digest":51,"rendered":52,"legacyId":61},{"title":46,"date":47,"description":48},"On Starting",["Date","2025-01-16T00:00:00.000Z"],"Why I'm keeping this journal","Every journey needs a first step, even if you don't know where you're going.\n\nThis journal is a place to document my explorations—both the philosophical questions I'm wrestling with and the technical problems I'm trying to solve. The two aren't as separate as they might seem. TaoCore grew out of reading the Tao Te Ching and wondering: what if software could find balance instead of chasing optima?\n\nI don't know what shape these entries will take. Some might be observations about human nature. Some might be notes on equilibrium mathematics. Some might be fragments that don't fit anywhere else.\n\nThe point isn't to produce polished essays. It's to think out loud, to notice patterns, to leave a trail of where my mind has wandered.\n\nLet's see where this goes.","src/content/journal/on-starting.md","eefa325bcf0576af",{"html":53,"metadata":54},"\u003Cp>Every journey needs a first step, even if you don’t know where you’re going.\u003C/p>\n\u003Cp>This journal is a place to document my explorations—both the philosophical questions I’m wrestling with and the technical problems I’m trying to solve. The two aren’t as separate as they might seem. TaoCore grew out of reading the Tao Te Ching and wondering: what if software could find balance instead of chasing optima?\u003C/p>\n\u003Cp>I don’t know what shape these entries will take. Some might be observations about human nature. Some might be notes on equilibrium mathematics. Some might be fragments that don’t fit anywhere else.\u003C/p>\n\u003Cp>The point isn’t to produce polished essays. It’s to think out loud, to notice patterns, to leave a trail of where my mind has wandered.\u003C/p>\n\u003Cp>Let’s see where this goes.\u003C/p>",{"headings":55,"localImagePaths":56,"remoteImagePaths":57,"frontmatter":58,"imagePaths":60},[],[],[],{"title":46,"date":59,"description":48},["Date","2025-01-16T00:00:00.000Z"],[],"on-starting.md","decomposing-the-observable-self",{"id":62,"data":64,"body":68,"filePath":69,"digest":70,"rendered":71,"legacyId":138},{"title":65,"date":66,"description":67},"Decomposing the Observable Self",["Date","2025-01-18T00:00:00.000Z"],"A framework for understanding what signals from images and video can tell us about human identity—and what they cannot.","How much of a person can be understood from what is visible? This question has occupied me as I work on systems that extract signals from images and video. The technology is advancing rapidly, but I find myself more interested in its limits than its capabilities.\n\n## The Inner-Outer Distinction\n\nWilliam James (1890) distinguished between the \"I\" (the knowing self) and the \"Me\" (the known self—the self as object). The \"Me\" includes the material self (body, possessions), the social self (recognition from others), and the spiritual self (inner thoughts, dispositions). Only portions of the \"Me\" are externally observable.\n\nGoffman (1959) extended this with dramaturgy: we perform versions of ourselves for different audiences. The \"front stage\" self—what others see—is curated, contextual, and strategic. The \"back stage\" self remains hidden.\n\n**What this means to me**: Any system that observes humans captures only performances, not essences. When I look at a photograph of someone, I'm seeing a moment they chose to present (or had presented for them). This isn't a limitation to overcome—it's a fundamental truth about what observation can access. I think we forget this too easily when building technology.\n\n## What Can Be Extracted from Images and Video\n\n### Face Detection: The Foundation\n\nModern face detection has achieved near-human performance under good conditions:\n\n| System/Study | Dataset | Accuracy | Notes |\n|-------------|---------|----------|-------|\n| RetinaFace (Deng et al., 2020) | WIDER FACE (hard) | 91.4% AP | State-of-the-art |\n| MTCNN (Zhang et al., 2016) | FDDB | 95.1% recall | Widely deployed |\n| Human performance | FDDB | ~94% | Baseline comparison |\n\nHowever, accuracy degrades significantly with:\n- **Occlusion**: 30-40% drop with partial face coverage (Ge et al., 2017)\n- **Pose variation**: 15-25% drop at extreme angles >60° (Zhu & Ramanan, 2012)\n- **Low resolution**: Below 20×20 pixels, detection drops to \u003C50% (Yang et al., 2016)\n- **Lighting**: Low-light conditions reduce accuracy by 20-35% (Li et al., 2019)\n\n**What this means**: The 91-95% accuracy numbers are misleading for real-world use. They come from benchmark conditions that don't reflect how people actually appear in photos and videos—partially obscured, badly lit, at odd angles. I've learned to mentally discount benchmark performance by 20-30% when thinking about practical applications.\n\n### Facial Landmark Detection\n\n68-point landmark models achieve:\n\n| Model | Dataset | NME (Normalized Mean Error) |\n|-------|---------|----------------------------|\n| HRNet (Sun et al., 2019) | 300W | 2.87% |\n| AWing (Wang et al., 2019) | WFLW | 4.36% |\n| Human inter-rater | 300W | ~3.0% |\n\n**What this means**: Landmark detection is genuinely good—approaching human-level agreement. This is the kind of geometric signal I trust. It's measuring shape, not meaning.\n\n### Body Pose Estimation\n\nSkeleton estimation accuracy (PCK @ 0.5 threshold—correct if within 50% of head size):\n\n| Model | Dataset | PCK@0.5 |\n|-------|---------|---------|\n| HRNet (Sun et al., 2019) | COCO | 76.3% |\n| OpenPose (Cao et al., 2019) | COCO | 65.3% |\n| AlphaPose (Fang et al., 2017) | COCO | 72.3% |\n\nMulti-person scenarios show 10-15% degradation due to occlusion and association errors.\n\n**What this means**: Pose estimation is useful but imperfect. 76% means roughly 1 in 4 joint positions is noticeably wrong. For measuring gross body position—standing, sitting, facing toward/away—it's adequate. For fine-grained gesture analysis, I'd be cautious.\n\n### The Problem with Emotion Recognition\n\nThis is where I become skeptical of the entire enterprise.\n\n**Claimed accuracy in controlled conditions:**\n\n| System | Dataset | Reported Accuracy |\n|--------|---------|-------------------|\n| Commercial API A | FER2013 | 71% |\n| Commercial API B | AffectNet | 65% |\n| Academic SOTA | RAF-DB | 88% |\n\n**But these numbers are misleading.** Barrett et al. (2019) conducted a meta-analysis of 1,000+ studies and found:\n\n- **Within-culture agreement** on facial expressions: r = 0.39 to 0.58\n- **Cross-cultural agreement**: drops to r = 0.20 to 0.45\n- **Reliability of \"basic emotion\" categories**: challenged by 25%+ disagreement even in posed expressions\n\nKey findings from their meta-analysis:\n\n| Expression | % Agreement (Western) | % Agreement (Cross-cultural) |\n|-----------|----------------------|------------------------------|\n| Happiness | 90% | 69% |\n| Sadness | 75% | 56% |\n| Anger | 74% | 59% |\n| Fear | 65% | 48% |\n| Disgust | 65% | 45% |\n| Surprise | 83% | 67% |\n\n**What this means to me**: This data changed how I think about the field. If humans only agree 48% of the time on what \"fear\" looks like across cultures, then any algorithm trained on Western-labeled data is encoding one culture's interpretation as universal truth. The 71% accuracy of a commercial API isn't measuring \"emotion detection\"—it's measuring agreement with Western labelers' interpretations of posed expressions.\n\nI find this troubling because these systems are being deployed as if they measure something real. But they're measuring agreement with a training set, not ground truth about human experience.\n\n### Facial Action Units: A More Honest Alternative\n\nThe Facial Action Coding System (FACS; Ekman & Friesen, 1978) describes muscle movements, not emotions:\n\n| AU | Description | Detection Accuracy (F1) |\n|----|-------------|------------------------|\n| AU1 | Inner brow raiser | 0.51 |\n| AU2 | Outer brow raiser | 0.45 |\n| AU4 | Brow lowerer | 0.55 |\n| AU6 | Cheek raiser | 0.76 |\n| AU12 | Lip corner puller | 0.86 |\n| AU15 | Lip corner depressor | 0.38 |\n| AU25 | Lips part | 0.92 |\n\nData from BP4D-Spontaneous dataset (Zhang et al., 2014).\n\n**What this means**: AUs are more honest because they describe what's actually observable—muscle movements—without claiming to know what they mean. AU12 (lip corner puller) is reliably detectable at 0.86 F1. Whether that \"smile\" indicates joy, politeness, nervousness, or a photographer's instruction is a separate question entirely.\n\nThis is the approach I want to take with taocore-human: describe the signal, not the interpretation.\n\n## Demographic Attributes: Where Accuracy Meets Ethics\n\n### Gender Classification Bias\n\nBuolamwini & Gebru (2018) tested three commercial systems on the Pilot Parliaments Benchmark:\n\n| Demographic Group | System A | System B | System C |\n|------------------|----------|----------|----------|\n| Lighter-skinned males | 0.8% error | 0.0% error | 0.0% error |\n| Lighter-skinned females | 6.0% error | 1.7% error | 7.1% error |\n| Darker-skinned males | 12.0% error | 0.7% error | 6.0% error |\n| Darker-skinned females | 34.7% error | 21.3% error | 34.5% error |\n\n**What this means**: Error rates for darker-skinned females were up to 43× higher than for lighter-skinned males. This isn't a bug to be fixed with more data—it reflects whose faces were considered worth including in training sets. The bias is structural.\n\n**My position**: I don't think we should build systems that classify gender from appearance. It conflates sex, gender identity, and gender expression. It fails for people who don't fit binary categories. And even when it \"works,\" it reinforces the idea that gender is something readable from the surface.\n\n### Age Estimation\n\nState-of-the-art mean absolute error (MAE) in years:\n\n| Model | MORPH II | FG-NET | UTKFace |\n|-------|----------|--------|---------|\n| DEX (Rothe et al., 2018) | 2.68 | 4.63 | - |\n| SSR-Net (Yang et al., 2018) | 3.16 | 4.48 | 5.21 |\n\nBut MAE varies significantly by actual age:\n\n| Age Group | Typical MAE |\n|-----------|-------------|\n| 0-20 | 3-5 years |\n| 20-40 | 4-6 years |\n| 40-60 | 5-8 years |\n| 60+ | 8-12 years |\n\nAnd by demographics (Ricanek & Tesafaye, 2006):\n- African American faces: +1.5 years MAE vs. Caucasian\n- Female faces: +0.8 years MAE vs. male (on Western-trained models)\n\n**What this means**: Age estimation is less politically charged than gender, but still unreliable. ±8 years for someone over 60 means the system might guess 55 or 70 for someone who is 63. If taocore-human ever reports age, it should be as a wide range with explicit uncertainty—\"apparent age: 55-70\"—not a point estimate.\n\n### Race Classification: Why I Won't Build It\n\nAccuracy seems high in benchmarks:\n\n| Study | Dataset | Accuracy |\n|-------|---------|----------|\n| Fu et al. (2014) | MORPH II (3 classes) | 99.1% |\n| Guo & Mu (2014) | PCSO (2 classes) | 98.3% |\n\nBut these benchmarks are fundamentally flawed:\n1. **Limited categories**: Most use 2-4 racial categories, erasing mixed-race and many ethnic groups\n2. **Labeling inconsistency**: Inter-rater agreement on race labels is only 85-90% (Albiero et al., 2020)\n3. **Downstream harm**: These systems have led to wrongful arrests (Hill, 2020)\n\n**My position**: Race is a social construct, not a biological category that can be read from faces. The high \"accuracy\" of these systems means they're good at reproducing the racial categories their training data encoded—not that race is something objectively measurable.\n\nI will not build race classification into taocore-human. Some capabilities shouldn't exist.\n\n## Reliability Across Conditions\n\nReal-world performance differs dramatically from benchmark performance:\n\n| Condition | Face Detection | Landmark | Pose | Expression |\n|-----------|---------------|----------|------|------------|\n| Lab conditions | 95%+ | 97%+ | 85%+ | 70%+ |\n| Good natural light | 90%+ | 92%+ | 75%+ | 55%+ |\n| Indoor variable | 80%+ | 85%+ | 65%+ | 45%+ |\n| Low light | 60%+ | 70%+ | 50%+ | 30%+ |\n| Motion blur | 55%+ | 60%+ | 45%+ | 25%+ |\n| Occlusion (>30%) | 50%+ | 55%+ | 40%+ | 20%+ |\n\n**What this means**: The gap between benchmark and reality is enormous. A system that's 95% accurate in the lab might be 50% accurate on your actual photos. This is why taocore-human needs to report confidence and refuse to interpret when conditions are poor. Silence is more honest than false confidence.\n\n## The Signal-to-Meaning Gap\n\nI think of this as a hierarchy where reliability degrades at each level:\n\n```\nLevel 1: Pixels (raw data)\n   ↓ [~5% information loss - compression, noise]\nLevel 2: Geometric features (faces, poses, positions)\n   ↓ [10-20% error - detection/estimation]\nLevel 3: Behavioral signals (AUs, pose configurations)\n   ↓ [30-50% error - context-dependent mapping]\nLevel 4: Psychological inference (emotions, intentions)\n   ↓ [50-70% error - weak construct validity]\nLevel 5: Identity claims (who someone \"is\")\n   ↓ [undefined - philosophical category error]\n```\n\n**My principle for taocore-human**: Operate at Levels 2-3. Report geometric features and behavioral signals with confidence intervals. Do not attempt Levels 4-5.\n\nThe temptation is always to climb higher—to say something more meaningful. But meaning without validity is noise dressed up as signal.\n\n## What Else Helps Us Understand Ourselves?\n\n### Voice and Speech\n\nParalinguistic features show moderate reliability:\n\n| Feature | Task | Accuracy/Correlation |\n|---------|------|---------------------|\n| Pitch variation | Arousal detection | r = 0.45-0.65 |\n| Speech rate | Stress detection | r = 0.35-0.50 |\n| Voice quality | Depression screening | AUC = 0.70-0.80 |\n\nData from Schuller & Batliner (2013).\n\n**What this means**: Voice carries real signal—more than I initially expected. The correlations with arousal and stress are moderate but meaningful. This might be a future direction for taocore-human, though audio raises its own privacy concerns.\n\n### Physiological Signals\n\n| Signal | What It Measures | Reliability (r with self-report) |\n|--------|-----------------|--------------------------------|\n| Heart rate variability | Arousal/stress | 0.40-0.60 |\n| Skin conductance | Emotional activation | 0.50-0.70 |\n| Pupil dilation | Cognitive load, arousal | 0.35-0.55 |\n\nData from Kreibig (2010) meta-analysis.\n\n**What this means**: Physiological signals correlate with self-reported states better than facial expressions do. This makes sense—they're measuring the body's actual response, not a culturally mediated display. But they require sensors, which limits practical application.\n\n## Framework for Ethical Signal Extraction\n\nBased on the data above, here's how I think about what taocore-human should and shouldn't do:\n\n| Signal Type | Reliability | Should Extract? | How to Report |\n|------------|-------------|-----------------|---------------|\n| Face presence/location | High (>90%) | Yes | Bounding box + confidence |\n| Facial landmarks | Moderate-High (85-95%) | Yes | Points + NME estimate |\n| Body pose | Moderate (65-80%) | Yes | Skeleton + PCK confidence |\n| Facial AUs | Variable (38-92%) | Selective | Only high-reliability AUs |\n| Emotion labels | Low (\u003C60%) | **No** | - |\n| Gender | Biased (65-99%) | **No** | - |\n| Race | Invalid construct | **No** | - |\n| Age | Moderate (±5-8 years) | With uncertainty | Range, not point estimate |\n\n## Conclusion: What I've Learned\n\nWorking through this literature has clarified my thinking:\n\n1. **Geometric signals are trustworthy.** Where faces are, how bodies are positioned—these are measurable with reasonable accuracy.\n\n2. **Behavioral signals require humility.** AUs and pose configurations can be detected, but what they mean depends on context we often don't have.\n\n3. **Psychological inferences are overreach.** The emotion recognition industry is built on shaky foundations. I don't want to contribute to it.\n\n4. **Identity categorizations are harmful.** Race and gender classification encode social constructs as biological facts. They should not be built.\n\nThe goal of taocore-human is not to understand people. It's to describe observable patterns while being honest about what remains unknown. The data tells us clearly: most of what matters about a person is not visible on the surface.\n\n---\n\n## References\n\nAlbiero, V., et al. (2020). Analysis of gender inequality in face recognition accuracy. *IEEE/CVF WACV Workshops*, 81-89.\n\nBarrett, L. F., et al. (2019). Emotional expressions reconsidered. *Psychological Science in the Public Interest*, 20(1), 1-68.\n\nBuolamwini, J., & Gebru, T. (2018). Gender shades. *FAT*, 77-91.\n\nCao, Z., et al. (2019). OpenPose. *IEEE TPAMI*, 43(1), 172-186.\n\nDeng, J., et al. (2020). RetinaFace. *IEEE/CVF CVPR*, 5203-5212.\n\nEkman, P., & Friesen, W. V. (1978). *Facial Action Coding System*. Consulting Psychologists Press.\n\nGoffman, E. (1959). *The Presentation of Self in Everyday Life*. Anchor Books.\n\nHill, K. (2020). Wrongfully accused by an algorithm. *The New York Times*, June 24.\n\nJames, W. (1890). *The Principles of Psychology*. Henry Holt.\n\nKreibig, S. D. (2010). Autonomic nervous system activity in emotion. *Biological Psychology*, 84(3), 394-421.\n\nRicanek, K., & Tesafaye, T. (2006). MORPH. *FG*, 341-345.\n\nRothe, R., et al. (2018). Deep expectation of real and apparent age. *IJCV*, 126(2-4), 144-157.\n\nSchuller, B., & Batliner, A. (2013). *Computational Paralinguistics*. Wiley.\n\nSun, K., et al. (2019). Deep high-resolution representation learning. *IEEE/CVF CVPR*, 5693-5703.\n\nZhang, K., et al. (2016). MTCNN. *IEEE SPL*, 23(10), 1499-1503.\n\nZhang, X., et al. (2014). BP4D-Spontaneous. *Image and Vision Computing*, 32(10), 692-706.","src/content/journal/decomposing-the-observable-self.md","a8fb99be12b31eeb",{"html":72,"metadata":73},"\u003Cp>How much of a person can be understood from what is visible? This question has occupied me as I work on systems that extract signals from images and video. The technology is advancing rapidly, but I find myself more interested in its limits than its capabilities.\u003C/p>\n\u003Ch2 id=\"the-inner-outer-distinction\">The Inner-Outer Distinction\u003C/h2>\n\u003Cp>William James (1890) distinguished between the “I” (the knowing self) and the “Me” (the known self—the self as object). The “Me” includes the material self (body, possessions), the social self (recognition from others), and the spiritual self (inner thoughts, dispositions). Only portions of the “Me” are externally observable.\u003C/p>\n\u003Cp>Goffman (1959) extended this with dramaturgy: we perform versions of ourselves for different audiences. The “front stage” self—what others see—is curated, contextual, and strategic. The “back stage” self remains hidden.\u003C/p>\n\u003Cp>\u003Cstrong>What this means to me\u003C/strong>: Any system that observes humans captures only performances, not essences. When I look at a photograph of someone, I’m seeing a moment they chose to present (or had presented for them). This isn’t a limitation to overcome—it’s a fundamental truth about what observation can access. I think we forget this too easily when building technology.\u003C/p>\n\u003Ch2 id=\"what-can-be-extracted-from-images-and-video\">What Can Be Extracted from Images and Video\u003C/h2>\n\u003Ch3 id=\"face-detection-the-foundation\">Face Detection: The Foundation\u003C/h3>\n\u003Cp>Modern face detection has achieved near-human performance under good conditions:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>System/Study\u003C/th>\u003Cth>Dataset\u003C/th>\u003Cth>Accuracy\u003C/th>\u003Cth>Notes\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>RetinaFace (Deng et al., 2020)\u003C/td>\u003Ctd>WIDER FACE (hard)\u003C/td>\u003Ctd>91.4% AP\u003C/td>\u003Ctd>State-of-the-art\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>MTCNN (Zhang et al., 2016)\u003C/td>\u003Ctd>FDDB\u003C/td>\u003Ctd>95.1% recall\u003C/td>\u003Ctd>Widely deployed\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Human performance\u003C/td>\u003Ctd>FDDB\u003C/td>\u003Ctd>~94%\u003C/td>\u003Ctd>Baseline comparison\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>However, accuracy degrades significantly with:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Occlusion\u003C/strong>: 30-40% drop with partial face coverage (Ge et al., 2017)\u003C/li>\n\u003Cli>\u003Cstrong>Pose variation\u003C/strong>: 15-25% drop at extreme angles >60° (Zhu &#x26; Ramanan, 2012)\u003C/li>\n\u003Cli>\u003Cstrong>Low resolution\u003C/strong>: Below 20×20 pixels, detection drops to &#x3C;50% (Yang et al., 2016)\u003C/li>\n\u003Cli>\u003Cstrong>Lighting\u003C/strong>: Low-light conditions reduce accuracy by 20-35% (Li et al., 2019)\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: The 91-95% accuracy numbers are misleading for real-world use. They come from benchmark conditions that don’t reflect how people actually appear in photos and videos—partially obscured, badly lit, at odd angles. I’ve learned to mentally discount benchmark performance by 20-30% when thinking about practical applications.\u003C/p>\n\u003Ch3 id=\"facial-landmark-detection\">Facial Landmark Detection\u003C/h3>\n\u003Cp>68-point landmark models achieve:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Model\u003C/th>\u003Cth>Dataset\u003C/th>\u003Cth>NME (Normalized Mean Error)\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>HRNet (Sun et al., 2019)\u003C/td>\u003Ctd>300W\u003C/td>\u003Ctd>2.87%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AWing (Wang et al., 2019)\u003C/td>\u003Ctd>WFLW\u003C/td>\u003Ctd>4.36%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Human inter-rater\u003C/td>\u003Ctd>300W\u003C/td>\u003Ctd>~3.0%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Landmark detection is genuinely good—approaching human-level agreement. This is the kind of geometric signal I trust. It’s measuring shape, not meaning.\u003C/p>\n\u003Ch3 id=\"body-pose-estimation\">Body Pose Estimation\u003C/h3>\n\u003Cp>Skeleton estimation accuracy (PCK @ 0.5 threshold—correct if within 50% of head size):\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Model\u003C/th>\u003Cth>Dataset\u003C/th>\u003Cth>PCK@0.5\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>HRNet (Sun et al., 2019)\u003C/td>\u003Ctd>COCO\u003C/td>\u003Ctd>76.3%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>OpenPose (Cao et al., 2019)\u003C/td>\u003Ctd>COCO\u003C/td>\u003Ctd>65.3%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AlphaPose (Fang et al., 2017)\u003C/td>\u003Ctd>COCO\u003C/td>\u003Ctd>72.3%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>Multi-person scenarios show 10-15% degradation due to occlusion and association errors.\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Pose estimation is useful but imperfect. 76% means roughly 1 in 4 joint positions is noticeably wrong. For measuring gross body position—standing, sitting, facing toward/away—it’s adequate. For fine-grained gesture analysis, I’d be cautious.\u003C/p>\n\u003Ch3 id=\"the-problem-with-emotion-recognition\">The Problem with Emotion Recognition\u003C/h3>\n\u003Cp>This is where I become skeptical of the entire enterprise.\u003C/p>\n\u003Cp>\u003Cstrong>Claimed accuracy in controlled conditions:\u003C/strong>\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>System\u003C/th>\u003Cth>Dataset\u003C/th>\u003Cth>Reported Accuracy\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Commercial API A\u003C/td>\u003Ctd>FER2013\u003C/td>\u003Ctd>71%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Commercial API B\u003C/td>\u003Ctd>AffectNet\u003C/td>\u003Ctd>65%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Academic SOTA\u003C/td>\u003Ctd>RAF-DB\u003C/td>\u003Ctd>88%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>But these numbers are misleading.\u003C/strong> Barrett et al. (2019) conducted a meta-analysis of 1,000+ studies and found:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Within-culture agreement\u003C/strong> on facial expressions: r = 0.39 to 0.58\u003C/li>\n\u003Cli>\u003Cstrong>Cross-cultural agreement\u003C/strong>: drops to r = 0.20 to 0.45\u003C/li>\n\u003Cli>\u003Cstrong>Reliability of “basic emotion” categories\u003C/strong>: challenged by 25%+ disagreement even in posed expressions\u003C/li>\n\u003C/ul>\n\u003Cp>Key findings from their meta-analysis:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Expression\u003C/th>\u003Cth>% Agreement (Western)\u003C/th>\u003Cth>% Agreement (Cross-cultural)\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Happiness\u003C/td>\u003Ctd>90%\u003C/td>\u003Ctd>69%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Sadness\u003C/td>\u003Ctd>75%\u003C/td>\u003Ctd>56%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Anger\u003C/td>\u003Ctd>74%\u003C/td>\u003Ctd>59%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Fear\u003C/td>\u003Ctd>65%\u003C/td>\u003Ctd>48%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Disgust\u003C/td>\u003Ctd>65%\u003C/td>\u003Ctd>45%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Surprise\u003C/td>\u003Ctd>83%\u003C/td>\u003Ctd>67%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>What this means to me\u003C/strong>: This data changed how I think about the field. If humans only agree 48% of the time on what “fear” looks like across cultures, then any algorithm trained on Western-labeled data is encoding one culture’s interpretation as universal truth. The 71% accuracy of a commercial API isn’t measuring “emotion detection”—it’s measuring agreement with Western labelers’ interpretations of posed expressions.\u003C/p>\n\u003Cp>I find this troubling because these systems are being deployed as if they measure something real. But they’re measuring agreement with a training set, not ground truth about human experience.\u003C/p>\n\u003Ch3 id=\"facial-action-units-a-more-honest-alternative\">Facial Action Units: A More Honest Alternative\u003C/h3>\n\u003Cp>The Facial Action Coding System (FACS; Ekman &#x26; Friesen, 1978) describes muscle movements, not emotions:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>AU\u003C/th>\u003Cth>Description\u003C/th>\u003Cth>Detection Accuracy (F1)\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>AU1\u003C/td>\u003Ctd>Inner brow raiser\u003C/td>\u003Ctd>0.51\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AU2\u003C/td>\u003Ctd>Outer brow raiser\u003C/td>\u003Ctd>0.45\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AU4\u003C/td>\u003Ctd>Brow lowerer\u003C/td>\u003Ctd>0.55\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AU6\u003C/td>\u003Ctd>Cheek raiser\u003C/td>\u003Ctd>0.76\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AU12\u003C/td>\u003Ctd>Lip corner puller\u003C/td>\u003Ctd>0.86\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AU15\u003C/td>\u003Ctd>Lip corner depressor\u003C/td>\u003Ctd>0.38\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>AU25\u003C/td>\u003Ctd>Lips part\u003C/td>\u003Ctd>0.92\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>Data from BP4D-Spontaneous dataset (Zhang et al., 2014).\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: AUs are more honest because they describe what’s actually observable—muscle movements—without claiming to know what they mean. AU12 (lip corner puller) is reliably detectable at 0.86 F1. Whether that “smile” indicates joy, politeness, nervousness, or a photographer’s instruction is a separate question entirely.\u003C/p>\n\u003Cp>This is the approach I want to take with taocore-human: describe the signal, not the interpretation.\u003C/p>\n\u003Ch2 id=\"demographic-attributes-where-accuracy-meets-ethics\">Demographic Attributes: Where Accuracy Meets Ethics\u003C/h2>\n\u003Ch3 id=\"gender-classification-bias\">Gender Classification Bias\u003C/h3>\n\u003Cp>Buolamwini &#x26; Gebru (2018) tested three commercial systems on the Pilot Parliaments Benchmark:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Demographic Group\u003C/th>\u003Cth>System A\u003C/th>\u003Cth>System B\u003C/th>\u003Cth>System C\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Lighter-skinned males\u003C/td>\u003Ctd>0.8% error\u003C/td>\u003Ctd>0.0% error\u003C/td>\u003Ctd>0.0% error\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Lighter-skinned females\u003C/td>\u003Ctd>6.0% error\u003C/td>\u003Ctd>1.7% error\u003C/td>\u003Ctd>7.1% error\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Darker-skinned males\u003C/td>\u003Ctd>12.0% error\u003C/td>\u003Ctd>0.7% error\u003C/td>\u003Ctd>6.0% error\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Darker-skinned females\u003C/td>\u003Ctd>34.7% error\u003C/td>\u003Ctd>21.3% error\u003C/td>\u003Ctd>34.5% error\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Error rates for darker-skinned females were up to 43× higher than for lighter-skinned males. This isn’t a bug to be fixed with more data—it reflects whose faces were considered worth including in training sets. The bias is structural.\u003C/p>\n\u003Cp>\u003Cstrong>My position\u003C/strong>: I don’t think we should build systems that classify gender from appearance. It conflates sex, gender identity, and gender expression. It fails for people who don’t fit binary categories. And even when it “works,” it reinforces the idea that gender is something readable from the surface.\u003C/p>\n\u003Ch3 id=\"age-estimation\">Age Estimation\u003C/h3>\n\u003Cp>State-of-the-art mean absolute error (MAE) in years:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Model\u003C/th>\u003Cth>MORPH II\u003C/th>\u003Cth>FG-NET\u003C/th>\u003Cth>UTKFace\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>DEX (Rothe et al., 2018)\u003C/td>\u003Ctd>2.68\u003C/td>\u003Ctd>4.63\u003C/td>\u003Ctd>-\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>SSR-Net (Yang et al., 2018)\u003C/td>\u003Ctd>3.16\u003C/td>\u003Ctd>4.48\u003C/td>\u003Ctd>5.21\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>But MAE varies significantly by actual age:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Age Group\u003C/th>\u003Cth>Typical MAE\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>0-20\u003C/td>\u003Ctd>3-5 years\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>20-40\u003C/td>\u003Ctd>4-6 years\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>40-60\u003C/td>\u003Ctd>5-8 years\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>60+\u003C/td>\u003Ctd>8-12 years\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>And by demographics (Ricanek &#x26; Tesafaye, 2006):\u003C/p>\n\u003Cul>\n\u003Cli>African American faces: +1.5 years MAE vs. Caucasian\u003C/li>\n\u003Cli>Female faces: +0.8 years MAE vs. male (on Western-trained models)\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Age estimation is less politically charged than gender, but still unreliable. ±8 years for someone over 60 means the system might guess 55 or 70 for someone who is 63. If taocore-human ever reports age, it should be as a wide range with explicit uncertainty—“apparent age: 55-70”—not a point estimate.\u003C/p>\n\u003Ch3 id=\"race-classification-why-i-wont-build-it\">Race Classification: Why I Won’t Build It\u003C/h3>\n\u003Cp>Accuracy seems high in benchmarks:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Study\u003C/th>\u003Cth>Dataset\u003C/th>\u003Cth>Accuracy\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Fu et al. (2014)\u003C/td>\u003Ctd>MORPH II (3 classes)\u003C/td>\u003Ctd>99.1%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Guo &#x26; Mu (2014)\u003C/td>\u003Ctd>PCSO (2 classes)\u003C/td>\u003Ctd>98.3%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>But these benchmarks are fundamentally flawed:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>Limited categories\u003C/strong>: Most use 2-4 racial categories, erasing mixed-race and many ethnic groups\u003C/li>\n\u003Cli>\u003Cstrong>Labeling inconsistency\u003C/strong>: Inter-rater agreement on race labels is only 85-90% (Albiero et al., 2020)\u003C/li>\n\u003Cli>\u003Cstrong>Downstream harm\u003C/strong>: These systems have led to wrongful arrests (Hill, 2020)\u003C/li>\n\u003C/ol>\n\u003Cp>\u003Cstrong>My position\u003C/strong>: Race is a social construct, not a biological category that can be read from faces. The high “accuracy” of these systems means they’re good at reproducing the racial categories their training data encoded—not that race is something objectively measurable.\u003C/p>\n\u003Cp>I will not build race classification into taocore-human. Some capabilities shouldn’t exist.\u003C/p>\n\u003Ch2 id=\"reliability-across-conditions\">Reliability Across Conditions\u003C/h2>\n\u003Cp>Real-world performance differs dramatically from benchmark performance:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Condition\u003C/th>\u003Cth>Face Detection\u003C/th>\u003Cth>Landmark\u003C/th>\u003Cth>Pose\u003C/th>\u003Cth>Expression\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Lab conditions\u003C/td>\u003Ctd>95%+\u003C/td>\u003Ctd>97%+\u003C/td>\u003Ctd>85%+\u003C/td>\u003Ctd>70%+\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Good natural light\u003C/td>\u003Ctd>90%+\u003C/td>\u003Ctd>92%+\u003C/td>\u003Ctd>75%+\u003C/td>\u003Ctd>55%+\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Indoor variable\u003C/td>\u003Ctd>80%+\u003C/td>\u003Ctd>85%+\u003C/td>\u003Ctd>65%+\u003C/td>\u003Ctd>45%+\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Low light\u003C/td>\u003Ctd>60%+\u003C/td>\u003Ctd>70%+\u003C/td>\u003Ctd>50%+\u003C/td>\u003Ctd>30%+\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Motion blur\u003C/td>\u003Ctd>55%+\u003C/td>\u003Ctd>60%+\u003C/td>\u003Ctd>45%+\u003C/td>\u003Ctd>25%+\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Occlusion (>30%)\u003C/td>\u003Ctd>50%+\u003C/td>\u003Ctd>55%+\u003C/td>\u003Ctd>40%+\u003C/td>\u003Ctd>20%+\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: The gap between benchmark and reality is enormous. A system that’s 95% accurate in the lab might be 50% accurate on your actual photos. This is why taocore-human needs to report confidence and refuse to interpret when conditions are poor. Silence is more honest than false confidence.\u003C/p>\n\u003Ch2 id=\"the-signal-to-meaning-gap\">The Signal-to-Meaning Gap\u003C/h2>\n\u003Cp>I think of this as a hierarchy where reliability degrades at each level:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Level 1: Pixels (raw data)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   ↓ [~5% information loss - compression, noise]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Level 2: Geometric features (faces, poses, positions)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   ↓ [10-20% error - detection/estimation]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Level 3: Behavioral signals (AUs, pose configurations)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   ↓ [30-50% error - context-dependent mapping]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Level 4: Psychological inference (emotions, intentions)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   ↓ [50-70% error - weak construct validity]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Level 5: Identity claims (who someone \"is\")\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   ↓ [undefined - philosophical category error]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>\u003Cstrong>My principle for taocore-human\u003C/strong>: Operate at Levels 2-3. Report geometric features and behavioral signals with confidence intervals. Do not attempt Levels 4-5.\u003C/p>\n\u003Cp>The temptation is always to climb higher—to say something more meaningful. But meaning without validity is noise dressed up as signal.\u003C/p>\n\u003Ch2 id=\"what-else-helps-us-understand-ourselves\">What Else Helps Us Understand Ourselves?\u003C/h2>\n\u003Ch3 id=\"voice-and-speech\">Voice and Speech\u003C/h3>\n\u003Cp>Paralinguistic features show moderate reliability:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Feature\u003C/th>\u003Cth>Task\u003C/th>\u003Cth>Accuracy/Correlation\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Pitch variation\u003C/td>\u003Ctd>Arousal detection\u003C/td>\u003Ctd>r = 0.45-0.65\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Speech rate\u003C/td>\u003Ctd>Stress detection\u003C/td>\u003Ctd>r = 0.35-0.50\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Voice quality\u003C/td>\u003Ctd>Depression screening\u003C/td>\u003Ctd>AUC = 0.70-0.80\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>Data from Schuller &#x26; Batliner (2013).\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Voice carries real signal—more than I initially expected. The correlations with arousal and stress are moderate but meaningful. This might be a future direction for taocore-human, though audio raises its own privacy concerns.\u003C/p>\n\u003Ch3 id=\"physiological-signals\">Physiological Signals\u003C/h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Signal\u003C/th>\u003Cth>What It Measures\u003C/th>\u003Cth>Reliability (r with self-report)\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Heart rate variability\u003C/td>\u003Ctd>Arousal/stress\u003C/td>\u003Ctd>0.40-0.60\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Skin conductance\u003C/td>\u003Ctd>Emotional activation\u003C/td>\u003Ctd>0.50-0.70\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Pupil dilation\u003C/td>\u003Ctd>Cognitive load, arousal\u003C/td>\u003Ctd>0.35-0.55\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>Data from Kreibig (2010) meta-analysis.\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Physiological signals correlate with self-reported states better than facial expressions do. This makes sense—they’re measuring the body’s actual response, not a culturally mediated display. But they require sensors, which limits practical application.\u003C/p>\n\u003Ch2 id=\"framework-for-ethical-signal-extraction\">Framework for Ethical Signal Extraction\u003C/h2>\n\u003Cp>Based on the data above, here’s how I think about what taocore-human should and shouldn’t do:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Signal Type\u003C/th>\u003Cth>Reliability\u003C/th>\u003Cth>Should Extract?\u003C/th>\u003Cth>How to Report\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Face presence/location\u003C/td>\u003Ctd>High (>90%)\u003C/td>\u003Ctd>Yes\u003C/td>\u003Ctd>Bounding box + confidence\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Facial landmarks\u003C/td>\u003Ctd>Moderate-High (85-95%)\u003C/td>\u003Ctd>Yes\u003C/td>\u003Ctd>Points + NME estimate\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Body pose\u003C/td>\u003Ctd>Moderate (65-80%)\u003C/td>\u003Ctd>Yes\u003C/td>\u003Ctd>Skeleton + PCK confidence\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Facial AUs\u003C/td>\u003Ctd>Variable (38-92%)\u003C/td>\u003Ctd>Selective\u003C/td>\u003Ctd>Only high-reliability AUs\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Emotion labels\u003C/td>\u003Ctd>Low (&#x3C;60%)\u003C/td>\u003Ctd>\u003Cstrong>No\u003C/strong>\u003C/td>\u003Ctd>-\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Gender\u003C/td>\u003Ctd>Biased (65-99%)\u003C/td>\u003Ctd>\u003Cstrong>No\u003C/strong>\u003C/td>\u003Ctd>-\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Race\u003C/td>\u003Ctd>Invalid construct\u003C/td>\u003Ctd>\u003Cstrong>No\u003C/strong>\u003C/td>\u003Ctd>-\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Age\u003C/td>\u003Ctd>Moderate (±5-8 years)\u003C/td>\u003Ctd>With uncertainty\u003C/td>\u003Ctd>Range, not point estimate\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch2 id=\"conclusion-what-ive-learned\">Conclusion: What I’ve Learned\u003C/h2>\n\u003Cp>Working through this literature has clarified my thinking:\u003C/p>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Geometric signals are trustworthy.\u003C/strong> Where faces are, how bodies are positioned—these are measurable with reasonable accuracy.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Behavioral signals require humility.\u003C/strong> AUs and pose configurations can be detected, but what they mean depends on context we often don’t have.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Psychological inferences are overreach.\u003C/strong> The emotion recognition industry is built on shaky foundations. I don’t want to contribute to it.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Identity categorizations are harmful.\u003C/strong> Race and gender classification encode social constructs as biological facts. They should not be built.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>The goal of taocore-human is not to understand people. It’s to describe observable patterns while being honest about what remains unknown. The data tells us clearly: most of what matters about a person is not visible on the surface.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"references\">References\u003C/h2>\n\u003Cp>Albiero, V., et al. (2020). Analysis of gender inequality in face recognition accuracy. \u003Cem>IEEE/CVF WACV Workshops\u003C/em>, 81-89.\u003C/p>\n\u003Cp>Barrett, L. F., et al. (2019). Emotional expressions reconsidered. \u003Cem>Psychological Science in the Public Interest\u003C/em>, 20(1), 1-68.\u003C/p>\n\u003Cp>Buolamwini, J., &#x26; Gebru, T. (2018). Gender shades. \u003Cem>FAT\u003C/em>, 77-91.\u003C/p>\n\u003Cp>Cao, Z., et al. (2019). OpenPose. \u003Cem>IEEE TPAMI\u003C/em>, 43(1), 172-186.\u003C/p>\n\u003Cp>Deng, J., et al. (2020). RetinaFace. \u003Cem>IEEE/CVF CVPR\u003C/em>, 5203-5212.\u003C/p>\n\u003Cp>Ekman, P., &#x26; Friesen, W. V. (1978). \u003Cem>Facial Action Coding System\u003C/em>. Consulting Psychologists Press.\u003C/p>\n\u003Cp>Goffman, E. (1959). \u003Cem>The Presentation of Self in Everyday Life\u003C/em>. Anchor Books.\u003C/p>\n\u003Cp>Hill, K. (2020). Wrongfully accused by an algorithm. \u003Cem>The New York Times\u003C/em>, June 24.\u003C/p>\n\u003Cp>James, W. (1890). \u003Cem>The Principles of Psychology\u003C/em>. Henry Holt.\u003C/p>\n\u003Cp>Kreibig, S. D. (2010). Autonomic nervous system activity in emotion. \u003Cem>Biological Psychology\u003C/em>, 84(3), 394-421.\u003C/p>\n\u003Cp>Ricanek, K., &#x26; Tesafaye, T. (2006). MORPH. \u003Cem>FG\u003C/em>, 341-345.\u003C/p>\n\u003Cp>Rothe, R., et al. (2018). Deep expectation of real and apparent age. \u003Cem>IJCV\u003C/em>, 126(2-4), 144-157.\u003C/p>\n\u003Cp>Schuller, B., &#x26; Batliner, A. (2013). \u003Cem>Computational Paralinguistics\u003C/em>. Wiley.\u003C/p>\n\u003Cp>Sun, K., et al. (2019). Deep high-resolution representation learning. \u003Cem>IEEE/CVF CVPR\u003C/em>, 5693-5703.\u003C/p>\n\u003Cp>Zhang, K., et al. (2016). MTCNN. \u003Cem>IEEE SPL\u003C/em>, 23(10), 1499-1503.\u003C/p>\n\u003Cp>Zhang, X., et al. (2014). BP4D-Spontaneous. \u003Cem>Image and Vision Computing\u003C/em>, 32(10), 692-706.\u003C/p>",{"headings":74,"localImagePaths":133,"remoteImagePaths":134,"frontmatter":135,"imagePaths":137},[75,78,81,85,88,91,94,97,100,103,106,109,112,115,118,121,124,127,130],{"depth":25,"slug":76,"text":77},"the-inner-outer-distinction","The Inner-Outer Distinction",{"depth":25,"slug":79,"text":80},"what-can-be-extracted-from-images-and-video","What Can Be Extracted from Images and Video",{"depth":82,"slug":83,"text":84},3,"face-detection-the-foundation","Face Detection: The Foundation",{"depth":82,"slug":86,"text":87},"facial-landmark-detection","Facial Landmark Detection",{"depth":82,"slug":89,"text":90},"body-pose-estimation","Body Pose Estimation",{"depth":82,"slug":92,"text":93},"the-problem-with-emotion-recognition","The Problem with Emotion Recognition",{"depth":82,"slug":95,"text":96},"facial-action-units-a-more-honest-alternative","Facial Action Units: A More Honest Alternative",{"depth":25,"slug":98,"text":99},"demographic-attributes-where-accuracy-meets-ethics","Demographic Attributes: Where Accuracy Meets Ethics",{"depth":82,"slug":101,"text":102},"gender-classification-bias","Gender Classification Bias",{"depth":82,"slug":104,"text":105},"age-estimation","Age Estimation",{"depth":82,"slug":107,"text":108},"race-classification-why-i-wont-build-it","Race Classification: Why I Won’t Build It",{"depth":25,"slug":110,"text":111},"reliability-across-conditions","Reliability Across Conditions",{"depth":25,"slug":113,"text":114},"the-signal-to-meaning-gap","The Signal-to-Meaning Gap",{"depth":25,"slug":116,"text":117},"what-else-helps-us-understand-ourselves","What Else Helps Us Understand Ourselves?",{"depth":82,"slug":119,"text":120},"voice-and-speech","Voice and Speech",{"depth":82,"slug":122,"text":123},"physiological-signals","Physiological Signals",{"depth":25,"slug":125,"text":126},"framework-for-ethical-signal-extraction","Framework for Ethical Signal Extraction",{"depth":25,"slug":128,"text":129},"conclusion-what-ive-learned","Conclusion: What I’ve Learned",{"depth":25,"slug":131,"text":132},"references","References",[],[],{"title":65,"date":136,"description":67},["Date","2025-01-18T00:00:00.000Z"],[],"decomposing-the-observable-self.md","the-relational-self",{"id":139,"data":141,"body":145,"filePath":146,"digest":147,"rendered":148,"legacyId":200},{"title":142,"date":143,"description":144},"The Relational Self: Where Do I End and We Begin?",["Date","2025-01-18T00:00:00.000Z"],"Exploring whether identity lives inside us or between us—through philosophy and research.","There's a question I keep returning to: **Is the self something I have, or something that happens between people?**\n\nWestern culture tells me I'm an individual first. My identity is inside me—my traits, my values, my essence. Relationships are things I *have*, not things I *am*.\n\nBut other traditions disagree. And so does a growing body of research.\n\n## Three Philosophical Views\n\n### The Western Individual\n\nDescartes started with \"I think, therefore I am\"—the self as a thinking thing, separate from world and others. This view runs deep in Western culture: identity is internal, stable, discoverable through introspection.\n\nFrom this perspective, I'm the same person whether I'm alone or with others. Context changes; I don't.\n\n**The appeal**: It gives us ownership of ourselves. Agency. Responsibility.\n\n**The problem**: It doesn't match how identity actually feels—shifting with context, shaped by relationships, never quite the same twice.\n\n### The Relational View\n\nBuddhist philosophy challenges the idea of a fixed self (*anatta*—\"non-self\"). What we call \"self\" is a process, not a thing. It arises in relationship and dissolves when conditions change.\n\nUbuntu philosophy from Southern Africa puts it directly: *\"I am because we are.\"* The self is constituted through community. You don't exist as a person outside of relationships.\n\nConfucian thought defines the self through roles—son, friend, citizen. To be a self is to be in right relationship with others.\n\n**The appeal**: It matches the experience of being shaped by the people around us.\n\n**The problem**: It can feel like losing yourself—if identity depends on others, who are you when alone?\n\n### The Process View\n\nHeraclitus said you can't step in the same river twice. The river is a pattern of flow, not a fixed thing.\n\nMaybe the self is similar. Not a thing to find, but a pattern to notice. It emerges in context, changes across time, and never holds still long enough to pin down.\n\n**The appeal**: It dissolves the either/or. Identity is neither purely internal nor purely external—it's a process that happens in the space between.\n\n**What this means to me**: I find this view most honest. I'm not the same person with everyone. That's not inauthenticity—it's what selves actually do.\n\n## What the Research Says\n\nPhilosophy gives us frameworks. Research gives us data.\n\n### We See Ourselves Through Others' Eyes\n\nCooley (1902) called this the \"looking-glass self\": we form our self-concept by imagining how others see us.\n\nShrauger & Schoeneman (1979) tested this across 60 studies:\n\n| Correlation | Strength |\n|-------------|----------|\n| Self-perception ↔ *perceived* other-perception | r = 0.40-0.60 |\n| Self-perception ↔ *actual* other-perception | r = 0.20-0.40 |\n\n**In plain terms**: We're more influenced by what we *imagine* others think than by what they actually think. Our self-image is built on imagination, not reality.\n\n**What this means**: The self isn't discovered through introspection—it's constructed through social imagination. The mirror we look into is partly made of projections.\n\n### Who We're With Shapes Who We Become\n\nEagle, Pentland, & Lazer (2009) tracked 94 people for 9 months using mobile phone data:\n\n| Signal | Correlation with self-reported friendship |\n|--------|------------------------------------------|\n| Face-to-face time | r = 0.52 |\n| Reciprocal calls | r = 0.58 |\n| Location overlap | r = 0.35 |\n\nCombined, proximity signals predicted friendship with 95% accuracy.\n\n**In plain terms**: Relationships aren't just feelings—they're patterns. Who you spend time with, and who reciprocates, predicts who you'd call a friend.\n\n**What this means**: Relationships are structural, not just emotional. And structure shapes identity. We become like the people we're around.\n\n### Network Position Predicts Outcomes\n\nBurt (1992, 2004) studied how social network position affects life outcomes:\n\n| Position | Effect |\n|----------|--------|\n| Bridging disconnected groups | Promoted 1.4 years earlier |\n| High-constraint (dense network) | 45% had ideas rated \"good\" |\n| Low-constraint (bridging) | 72% had ideas rated \"good\" |\n\n**In plain terms**: Where you sit in the social network—not just who you know, but how those people connect to each other—predicts success and creativity.\n\n**What this means**: Identity isn't just about who you are, but where you are in the web of relationships. Position shapes possibility.\n\n### Social Networks Seek Balance\n\nHeider (1958) proposed that relationships tend toward equilibrium. Leskovec et al. (2010) tested this on 100,000+ users:\n\n| Triad type | Expected | Observed |\n|------------|----------|----------|\n| All friends (+++) | 12.5% | 35.2% |\n| Two friends, one enemy (++-) | 37.5% | 38.1% |\n| One friend, two enemies (+--) | 37.5% | 21.8% |\n| All enemies (---) | 12.5% | 4.9% |\n\n73% of triads were balanced (vs. 50% expected by chance).\n\n**In plain terms**: \"The friend of my friend is my friend\" isn't just folk wisdom—it's statistically measurable. Social networks evolve toward stable configurations.\n\n**What this means**: There's pressure toward coherence in our relationships. Navigating between disconnected worlds creates structural tension, not just emotional tension.\n\n### Your Relational Style Is Stable\n\nSaramäki et al. (2014) tracked how people maintain relationships over 18 months:\n\n| Property | Stability |\n|----------|-----------|\n| Network size | r = 0.95 |\n| Tie strength distribution | r = 0.87 |\n| Top 5 contacts persistence | 72% |\n\n**In plain terms**: The *shape* of how you distribute attention across relationships stays remarkably stable, even as specific relationships change.\n\n**What this means**: We each have a relational signature—a pattern of how we connect. Understanding this pattern might be as revealing as understanding personality traits.\n\nDunbar (1992) identified consistent layers:\n\n| Circle | Typical size | Contact frequency |\n|--------|--------------|-------------------|\n| Support clique | 5 | Weekly |\n| Sympathy group | 15 | Monthly |\n| Active network | 150 | Yearly |\n\nThese layers appear across cultures. We seem to have cognitive limits on how many relationships we can maintain at different depths.\n\n## The Privacy Problem\n\nThis research has a shadow side.\n\nJernigan & Mistree (2009) showed that social network patterns can reveal things people haven't disclosed:\n- Sexual orientation predictable from friend networks (AUC = 0.78)\n- No explicit disclosure required\n\n**What this means**: Relationship data is sensitive data. Patterns speak even when people don't. The power to observe relationships is the power to expose what people might want to keep private.\n\nThis shapes how I think about taocore-human. Detecting relational patterns isn't neutral. It carries responsibility.\n\n## So Are My Relationships Part of Me?\n\nAfter sitting with this question:\n\n**My relationships are part of me the way language is.** I didn't choose Mandarin or English—they were given to me by context. But they shape how I think, what I can express, who I can connect with. They're not external to me; they're woven into how I experience the world.\n\n**The different versions of me are all real.** I'm not the same person in every context. That's not performance or inauthenticity—it's what selves do. The philosopher William James said we have as many social selves as there are people who recognize us. I think he was right.\n\n**The tension is also me.** If you move between different worlds—cultures, communities, generations—you feel the pull of multiple belongings. That tension isn't a failure of identity. It's what identity feels like when it's relational.\n\n## What This Means for taocore-human\n\nFor the system I'm building:\n\n**Extract** (with uncertainty):\n- Co-occurrence patterns\n- Network centrality\n- Proximity signals\n- Synchrony indicators\n\n**Report** (honestly):\n- Numbers with confidence intervals\n- Pattern descriptions, not relationship labels\n- What cannot be determined\n\n**Refuse**:\n- Relationship type classifications\n- Quality judgments\n- Predictions about individuals\n- Anything that could expose what people haven't chosen to reveal\n\n## A Closing Thought\n\nThe question \"who am I?\" might be less useful than \"who am I with whom?\"\n\nIdentity isn't a thing to find. It's a pattern that emerges in relationship—with others, with context, with time. Observable behavior traces this pattern imperfectly. What we see is real, but it's not the whole.\n\nThe reflection on the water is not the water.\n\n---\n\n## References\n\nBurt, R. S. (1992). *Structural Holes*. Harvard University Press.\n\nBurt, R. S. (2004). Structural holes and good ideas. *American Journal of Sociology*, 110(2), 349-399.\n\nCooley, C. H. (1902). *Human Nature and the Social Order*. Scribner's.\n\nDunbar, R. I. M. (1992). Neocortex size as a constraint on group size. *Journal of Human Evolution*, 22(6), 469-493.\n\nEagle, N., Pentland, A., & Lazer, D. (2009). Inferring friendship network structure. *PNAS*, 106(36), 15274-15278.\n\nHeider, F. (1958). *The Psychology of Interpersonal Relations*. Wiley.\n\nJernigan, C., & Mistree, B. F. (2009). Gaydar: Facebook friendships expose sexual orientation. *First Monday*, 14(10).\n\nLeskovec, J., et al. (2010). Predicting positive and negative links in online social networks. *WWW*, 641-650.\n\nSaramäki, J., et al. (2014). Persistence of social signatures in human communication. *PNAS*, 111(3), 942-947.\n\nShrauger, J. S., & Schoeneman, T. J. (1979). Symbolic interactionist view of self-concept. *Psychological Bulletin*, 86(3), 549-573.","src/content/journal/the-relational-self.md","16e686dcf1b958a6",{"html":149,"metadata":150},"\u003Cp>There’s a question I keep returning to: \u003Cstrong>Is the self something I have, or something that happens between people?\u003C/strong>\u003C/p>\n\u003Cp>Western culture tells me I’m an individual first. My identity is inside me—my traits, my values, my essence. Relationships are things I \u003Cem>have\u003C/em>, not things I \u003Cem>am\u003C/em>.\u003C/p>\n\u003Cp>But other traditions disagree. And so does a growing body of research.\u003C/p>\n\u003Ch2 id=\"three-philosophical-views\">Three Philosophical Views\u003C/h2>\n\u003Ch3 id=\"the-western-individual\">The Western Individual\u003C/h3>\n\u003Cp>Descartes started with “I think, therefore I am”—the self as a thinking thing, separate from world and others. This view runs deep in Western culture: identity is internal, stable, discoverable through introspection.\u003C/p>\n\u003Cp>From this perspective, I’m the same person whether I’m alone or with others. Context changes; I don’t.\u003C/p>\n\u003Cp>\u003Cstrong>The appeal\u003C/strong>: It gives us ownership of ourselves. Agency. Responsibility.\u003C/p>\n\u003Cp>\u003Cstrong>The problem\u003C/strong>: It doesn’t match how identity actually feels—shifting with context, shaped by relationships, never quite the same twice.\u003C/p>\n\u003Ch3 id=\"the-relational-view\">The Relational View\u003C/h3>\n\u003Cp>Buddhist philosophy challenges the idea of a fixed self (\u003Cem>anatta\u003C/em>—“non-self”). What we call “self” is a process, not a thing. It arises in relationship and dissolves when conditions change.\u003C/p>\n\u003Cp>Ubuntu philosophy from Southern Africa puts it directly: \u003Cem>“I am because we are.”\u003C/em> The self is constituted through community. You don’t exist as a person outside of relationships.\u003C/p>\n\u003Cp>Confucian thought defines the self through roles—son, friend, citizen. To be a self is to be in right relationship with others.\u003C/p>\n\u003Cp>\u003Cstrong>The appeal\u003C/strong>: It matches the experience of being shaped by the people around us.\u003C/p>\n\u003Cp>\u003Cstrong>The problem\u003C/strong>: It can feel like losing yourself—if identity depends on others, who are you when alone?\u003C/p>\n\u003Ch3 id=\"the-process-view\">The Process View\u003C/h3>\n\u003Cp>Heraclitus said you can’t step in the same river twice. The river is a pattern of flow, not a fixed thing.\u003C/p>\n\u003Cp>Maybe the self is similar. Not a thing to find, but a pattern to notice. It emerges in context, changes across time, and never holds still long enough to pin down.\u003C/p>\n\u003Cp>\u003Cstrong>The appeal\u003C/strong>: It dissolves the either/or. Identity is neither purely internal nor purely external—it’s a process that happens in the space between.\u003C/p>\n\u003Cp>\u003Cstrong>What this means to me\u003C/strong>: I find this view most honest. I’m not the same person with everyone. That’s not inauthenticity—it’s what selves actually do.\u003C/p>\n\u003Ch2 id=\"what-the-research-says\">What the Research Says\u003C/h2>\n\u003Cp>Philosophy gives us frameworks. Research gives us data.\u003C/p>\n\u003Ch3 id=\"we-see-ourselves-through-others-eyes\">We See Ourselves Through Others’ Eyes\u003C/h3>\n\u003Cp>Cooley (1902) called this the “looking-glass self”: we form our self-concept by imagining how others see us.\u003C/p>\n\u003Cp>Shrauger &#x26; Schoeneman (1979) tested this across 60 studies:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Correlation\u003C/th>\u003Cth>Strength\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Self-perception ↔ \u003Cem>perceived\u003C/em> other-perception\u003C/td>\u003Ctd>r = 0.40-0.60\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Self-perception ↔ \u003Cem>actual\u003C/em> other-perception\u003C/td>\u003Ctd>r = 0.20-0.40\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>In plain terms\u003C/strong>: We’re more influenced by what we \u003Cem>imagine\u003C/em> others think than by what they actually think. Our self-image is built on imagination, not reality.\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: The self isn’t discovered through introspection—it’s constructed through social imagination. The mirror we look into is partly made of projections.\u003C/p>\n\u003Ch3 id=\"who-were-with-shapes-who-we-become\">Who We’re With Shapes Who We Become\u003C/h3>\n\u003Cp>Eagle, Pentland, &#x26; Lazer (2009) tracked 94 people for 9 months using mobile phone data:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Signal\u003C/th>\u003Cth>Correlation with self-reported friendship\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Face-to-face time\u003C/td>\u003Ctd>r = 0.52\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Reciprocal calls\u003C/td>\u003Ctd>r = 0.58\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Location overlap\u003C/td>\u003Ctd>r = 0.35\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>Combined, proximity signals predicted friendship with 95% accuracy.\u003C/p>\n\u003Cp>\u003Cstrong>In plain terms\u003C/strong>: Relationships aren’t just feelings—they’re patterns. Who you spend time with, and who reciprocates, predicts who you’d call a friend.\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Relationships are structural, not just emotional. And structure shapes identity. We become like the people we’re around.\u003C/p>\n\u003Ch3 id=\"network-position-predicts-outcomes\">Network Position Predicts Outcomes\u003C/h3>\n\u003Cp>Burt (1992, 2004) studied how social network position affects life outcomes:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Position\u003C/th>\u003Cth>Effect\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Bridging disconnected groups\u003C/td>\u003Ctd>Promoted 1.4 years earlier\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>High-constraint (dense network)\u003C/td>\u003Ctd>45% had ideas rated “good”\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Low-constraint (bridging)\u003C/td>\u003Ctd>72% had ideas rated “good”\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>In plain terms\u003C/strong>: Where you sit in the social network—not just who you know, but how those people connect to each other—predicts success and creativity.\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Identity isn’t just about who you are, but where you are in the web of relationships. Position shapes possibility.\u003C/p>\n\u003Ch3 id=\"social-networks-seek-balance\">Social Networks Seek Balance\u003C/h3>\n\u003Cp>Heider (1958) proposed that relationships tend toward equilibrium. Leskovec et al. (2010) tested this on 100,000+ users:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Triad type\u003C/th>\u003Cth>Expected\u003C/th>\u003Cth>Observed\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>All friends (+++)\u003C/td>\u003Ctd>12.5%\u003C/td>\u003Ctd>35.2%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Two friends, one enemy (++-)\u003C/td>\u003Ctd>37.5%\u003C/td>\u003Ctd>38.1%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>One friend, two enemies (+—)\u003C/td>\u003Ctd>37.5%\u003C/td>\u003Ctd>21.8%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>All enemies (---)\u003C/td>\u003Ctd>12.5%\u003C/td>\u003Ctd>4.9%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>73% of triads were balanced (vs. 50% expected by chance).\u003C/p>\n\u003Cp>\u003Cstrong>In plain terms\u003C/strong>: “The friend of my friend is my friend” isn’t just folk wisdom—it’s statistically measurable. Social networks evolve toward stable configurations.\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: There’s pressure toward coherence in our relationships. Navigating between disconnected worlds creates structural tension, not just emotional tension.\u003C/p>\n\u003Ch3 id=\"your-relational-style-is-stable\">Your Relational Style Is Stable\u003C/h3>\n\u003Cp>Saramäki et al. (2014) tracked how people maintain relationships over 18 months:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Property\u003C/th>\u003Cth>Stability\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Network size\u003C/td>\u003Ctd>r = 0.95\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Tie strength distribution\u003C/td>\u003Ctd>r = 0.87\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Top 5 contacts persistence\u003C/td>\u003Ctd>72%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>In plain terms\u003C/strong>: The \u003Cem>shape\u003C/em> of how you distribute attention across relationships stays remarkably stable, even as specific relationships change.\u003C/p>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: We each have a relational signature—a pattern of how we connect. Understanding this pattern might be as revealing as understanding personality traits.\u003C/p>\n\u003Cp>Dunbar (1992) identified consistent layers:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Circle\u003C/th>\u003Cth>Typical size\u003C/th>\u003Cth>Contact frequency\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Support clique\u003C/td>\u003Ctd>5\u003C/td>\u003Ctd>Weekly\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Sympathy group\u003C/td>\u003Ctd>15\u003C/td>\u003Ctd>Monthly\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Active network\u003C/td>\u003Ctd>150\u003C/td>\u003Ctd>Yearly\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>These layers appear across cultures. We seem to have cognitive limits on how many relationships we can maintain at different depths.\u003C/p>\n\u003Ch2 id=\"the-privacy-problem\">The Privacy Problem\u003C/h2>\n\u003Cp>This research has a shadow side.\u003C/p>\n\u003Cp>Jernigan &#x26; Mistree (2009) showed that social network patterns can reveal things people haven’t disclosed:\u003C/p>\n\u003Cul>\n\u003Cli>Sexual orientation predictable from friend networks (AUC = 0.78)\u003C/li>\n\u003Cli>No explicit disclosure required\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>What this means\u003C/strong>: Relationship data is sensitive data. Patterns speak even when people don’t. The power to observe relationships is the power to expose what people might want to keep private.\u003C/p>\n\u003Cp>This shapes how I think about taocore-human. Detecting relational patterns isn’t neutral. It carries responsibility.\u003C/p>\n\u003Ch2 id=\"so-are-my-relationships-part-of-me\">So Are My Relationships Part of Me?\u003C/h2>\n\u003Cp>After sitting with this question:\u003C/p>\n\u003Cp>\u003Cstrong>My relationships are part of me the way language is.\u003C/strong> I didn’t choose Mandarin or English—they were given to me by context. But they shape how I think, what I can express, who I can connect with. They’re not external to me; they’re woven into how I experience the world.\u003C/p>\n\u003Cp>\u003Cstrong>The different versions of me are all real.\u003C/strong> I’m not the same person in every context. That’s not performance or inauthenticity—it’s what selves do. The philosopher William James said we have as many social selves as there are people who recognize us. I think he was right.\u003C/p>\n\u003Cp>\u003Cstrong>The tension is also me.\u003C/strong> If you move between different worlds—cultures, communities, generations—you feel the pull of multiple belongings. That tension isn’t a failure of identity. It’s what identity feels like when it’s relational.\u003C/p>\n\u003Ch2 id=\"what-this-means-for-taocore-human\">What This Means for taocore-human\u003C/h2>\n\u003Cp>For the system I’m building:\u003C/p>\n\u003Cp>\u003Cstrong>Extract\u003C/strong> (with uncertainty):\u003C/p>\n\u003Cul>\n\u003Cli>Co-occurrence patterns\u003C/li>\n\u003Cli>Network centrality\u003C/li>\n\u003Cli>Proximity signals\u003C/li>\n\u003Cli>Synchrony indicators\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Report\u003C/strong> (honestly):\u003C/p>\n\u003Cul>\n\u003Cli>Numbers with confidence intervals\u003C/li>\n\u003Cli>Pattern descriptions, not relationship labels\u003C/li>\n\u003Cli>What cannot be determined\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Refuse\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Relationship type classifications\u003C/li>\n\u003Cli>Quality judgments\u003C/li>\n\u003Cli>Predictions about individuals\u003C/li>\n\u003Cli>Anything that could expose what people haven’t chosen to reveal\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"a-closing-thought\">A Closing Thought\u003C/h2>\n\u003Cp>The question “who am I?” might be less useful than “who am I with whom?”\u003C/p>\n\u003Cp>Identity isn’t a thing to find. It’s a pattern that emerges in relationship—with others, with context, with time. Observable behavior traces this pattern imperfectly. What we see is real, but it’s not the whole.\u003C/p>\n\u003Cp>The reflection on the water is not the water.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"references\">References\u003C/h2>\n\u003Cp>Burt, R. S. (1992). \u003Cem>Structural Holes\u003C/em>. Harvard University Press.\u003C/p>\n\u003Cp>Burt, R. S. (2004). Structural holes and good ideas. \u003Cem>American Journal of Sociology\u003C/em>, 110(2), 349-399.\u003C/p>\n\u003Cp>Cooley, C. H. (1902). \u003Cem>Human Nature and the Social Order\u003C/em>. Scribner’s.\u003C/p>\n\u003Cp>Dunbar, R. I. M. (1992). Neocortex size as a constraint on group size. \u003Cem>Journal of Human Evolution\u003C/em>, 22(6), 469-493.\u003C/p>\n\u003Cp>Eagle, N., Pentland, A., &#x26; Lazer, D. (2009). Inferring friendship network structure. \u003Cem>PNAS\u003C/em>, 106(36), 15274-15278.\u003C/p>\n\u003Cp>Heider, F. (1958). \u003Cem>The Psychology of Interpersonal Relations\u003C/em>. Wiley.\u003C/p>\n\u003Cp>Jernigan, C., &#x26; Mistree, B. F. (2009). Gaydar: Facebook friendships expose sexual orientation. \u003Cem>First Monday\u003C/em>, 14(10).\u003C/p>\n\u003Cp>Leskovec, J., et al. (2010). Predicting positive and negative links in online social networks. \u003Cem>WWW\u003C/em>, 641-650.\u003C/p>\n\u003Cp>Saramäki, J., et al. (2014). Persistence of social signatures in human communication. \u003Cem>PNAS\u003C/em>, 111(3), 942-947.\u003C/p>\n\u003Cp>Shrauger, J. S., &#x26; Schoeneman, T. J. (1979). Symbolic interactionist view of self-concept. \u003Cem>Psychological Bulletin\u003C/em>, 86(3), 549-573.\u003C/p>",{"headings":151,"localImagePaths":195,"remoteImagePaths":196,"frontmatter":197,"imagePaths":199},[152,155,158,161,164,167,170,173,176,179,182,185,188,191,194],{"depth":25,"slug":153,"text":154},"three-philosophical-views","Three Philosophical Views",{"depth":82,"slug":156,"text":157},"the-western-individual","The Western Individual",{"depth":82,"slug":159,"text":160},"the-relational-view","The Relational View",{"depth":82,"slug":162,"text":163},"the-process-view","The Process View",{"depth":25,"slug":165,"text":166},"what-the-research-says","What the Research Says",{"depth":82,"slug":168,"text":169},"we-see-ourselves-through-others-eyes","We See Ourselves Through Others’ Eyes",{"depth":82,"slug":171,"text":172},"who-were-with-shapes-who-we-become","Who We’re With Shapes Who We Become",{"depth":82,"slug":174,"text":175},"network-position-predicts-outcomes","Network Position Predicts Outcomes",{"depth":82,"slug":177,"text":178},"social-networks-seek-balance","Social Networks Seek Balance",{"depth":82,"slug":180,"text":181},"your-relational-style-is-stable","Your Relational Style Is Stable",{"depth":25,"slug":183,"text":184},"the-privacy-problem","The Privacy Problem",{"depth":25,"slug":186,"text":187},"so-are-my-relationships-part-of-me","So Are My Relationships Part of Me?",{"depth":25,"slug":189,"text":190},"what-this-means-for-taocore-human","What This Means for taocore-human",{"depth":25,"slug":192,"text":193},"a-closing-thought","A Closing Thought",{"depth":25,"slug":131,"text":132},[],[],{"title":142,"date":198,"description":144},["Date","2025-01-18T00:00:00.000Z"],[],"the-relational-self.md","on-what-photos-remember",{"id":201,"data":203,"body":207,"filePath":208,"digest":209,"rendered":210,"legacyId":237},{"title":204,"date":205,"description":206},"On What Photos Remember",["Date","2025-01-30T00:00:00.000Z"],"What can a smartphone photo actually tell us about ourselves?","Every photo taken on a smartphone carries more than the image itself. There's the visible—faces, places, moments frozen in pixels. And there's the invisible—metadata whispered into the file, signals that machines can extract, patterns that emerge only when you look across many images at once.\n\nI've been building tools to surface these signals. Not to judge them, but to see them. The question that keeps returning: what can this information actually teach us about ourselves?\n\n## What a single photo contains\n\nThe raw data is surprisingly rich:\n\n**Metadata from the device**\n- When the photo was taken (date, time, sometimes down to the second)\n- Where it was taken (GPS coordinates, if location services were on)\n- What device captured it (camera make, model, lens characteristics)\n- Technical settings (orientation, sometimes exposure data)\n\n**Signals extractable from the image**\n- Faces present: how many, where positioned, relative sizes\n- Facial geometry: eye openness, mouth shape, head angle, gaze direction\n- Body poses: posture, arm positions, spatial relationships between people\n- Scene context: indoor or outdoor, time of day (from lighting), general environment type\n\n**What emerges from patterns**\n- Who appears frequently in your photos\n- Where you tend to be, and when\n- How your expressions cluster over time\n- The rhythm of your photography itself—bursts and silences\n\n## The limits of extraction\n\nNone of this tells us what matters most. A smile detected at 73% intensity doesn't mean happiness. A photo taken at 2am doesn't mean insomnia—it might mean a celebration, a feeding a newborn, or a red-eye flight. GPS coordinates in Paris don't capture whether you were lonely or in love.\n\nThe signals are geometric, temporal, statistical. They describe surfaces, not depths.\n\nAnd yet.\n\n## Where self-knowledge might live\n\nThe value isn't in any single measurement. It's in the aggregate, viewed with honest eyes.\n\nIf I look at a year of photos and notice:\n- I rarely appear in my own photos\n- The faces that appear most often are colleagues, not friends\n- Most photos are taken in transit, few at rest\n- My expressions in group photos differ from solo selfies\n\nThese patterns don't tell me what to feel. They show me what I've been doing. The interpretation—the meaning—that's mine to make.\n\nThere's something Taoist in this approach. The data doesn't push; it reflects. Like water showing you your own face, it offers no opinion. You bring the questions; the patterns suggest where to look for answers.\n\n## A tool for reflection, not judgment\n\nThe danger is obvious: these tools can be used to surveil, to categorize, to reduce people to profiles. I don't want to build that.\n\nWhat I'm trying to build is a mirror. One that shows you patterns in your own life, patterns you might not notice because you're too close to see them. Not to tell you what's wrong, but to prompt the question: *is this what I intended?*\n\nThe photo library on your phone is an accidental diary. Years of moments, accumulating. Most of us never read it. We scroll past, looking for a specific memory, never asking what the collection as a whole might reveal.\n\n## Questions worth asking\n\nIf you could see the aggregate truth of your photos:\n- Who do you photograph most? Is that who you want to remember?\n- Where do your photos cluster? Is that where your life is, or where you wish it was?\n- When do you take photos? What prompts you to capture a moment?\n- How do your expressions change across contexts? With whom do you seem most at ease?\n\nThese aren't questions a machine can answer. But a machine can surface the data that makes the questions possible.\n\n## The practice\n\nI don't think this kind of reflection should be automated. No app should tell you \"you seem happier on weekends\" or \"you don't photograph your family enough.\" That crosses from observation into judgment.\n\nInstead, the practice might be simpler: periodically look at what the signals say. Sit with the patterns. Notice what surprises you. Ask yourself why.\n\nThe photo knows when and where. The face detection knows expressions and positions. Only you know what it meant.\n\n---\n\n*This thinking led to the [image analysis tool](/analyze) on this site—an experiment in surfacing signals without interpreting them. Upload a photo, see what's extractable. The meaning remains yours to find.*","src/content/journal/on-what-photos-remember.md","98924e23d07407ad",{"html":211,"metadata":212},"\u003Cp>Every photo taken on a smartphone carries more than the image itself. There’s the visible—faces, places, moments frozen in pixels. And there’s the invisible—metadata whispered into the file, signals that machines can extract, patterns that emerge only when you look across many images at once.\u003C/p>\n\u003Cp>I’ve been building tools to surface these signals. Not to judge them, but to see them. The question that keeps returning: what can this information actually teach us about ourselves?\u003C/p>\n\u003Ch2 id=\"what-a-single-photo-contains\">What a single photo contains\u003C/h2>\n\u003Cp>The raw data is surprisingly rich:\u003C/p>\n\u003Cp>\u003Cstrong>Metadata from the device\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>When the photo was taken (date, time, sometimes down to the second)\u003C/li>\n\u003Cli>Where it was taken (GPS coordinates, if location services were on)\u003C/li>\n\u003Cli>What device captured it (camera make, model, lens characteristics)\u003C/li>\n\u003Cli>Technical settings (orientation, sometimes exposure data)\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Signals extractable from the image\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Faces present: how many, where positioned, relative sizes\u003C/li>\n\u003Cli>Facial geometry: eye openness, mouth shape, head angle, gaze direction\u003C/li>\n\u003Cli>Body poses: posture, arm positions, spatial relationships between people\u003C/li>\n\u003Cli>Scene context: indoor or outdoor, time of day (from lighting), general environment type\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>What emerges from patterns\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Who appears frequently in your photos\u003C/li>\n\u003Cli>Where you tend to be, and when\u003C/li>\n\u003Cli>How your expressions cluster over time\u003C/li>\n\u003Cli>The rhythm of your photography itself—bursts and silences\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"the-limits-of-extraction\">The limits of extraction\u003C/h2>\n\u003Cp>None of this tells us what matters most. A smile detected at 73% intensity doesn’t mean happiness. A photo taken at 2am doesn’t mean insomnia—it might mean a celebration, a feeding a newborn, or a red-eye flight. GPS coordinates in Paris don’t capture whether you were lonely or in love.\u003C/p>\n\u003Cp>The signals are geometric, temporal, statistical. They describe surfaces, not depths.\u003C/p>\n\u003Cp>And yet.\u003C/p>\n\u003Ch2 id=\"where-self-knowledge-might-live\">Where self-knowledge might live\u003C/h2>\n\u003Cp>The value isn’t in any single measurement. It’s in the aggregate, viewed with honest eyes.\u003C/p>\n\u003Cp>If I look at a year of photos and notice:\u003C/p>\n\u003Cul>\n\u003Cli>I rarely appear in my own photos\u003C/li>\n\u003Cli>The faces that appear most often are colleagues, not friends\u003C/li>\n\u003Cli>Most photos are taken in transit, few at rest\u003C/li>\n\u003Cli>My expressions in group photos differ from solo selfies\u003C/li>\n\u003C/ul>\n\u003Cp>These patterns don’t tell me what to feel. They show me what I’ve been doing. The interpretation—the meaning—that’s mine to make.\u003C/p>\n\u003Cp>There’s something Taoist in this approach. The data doesn’t push; it reflects. Like water showing you your own face, it offers no opinion. You bring the questions; the patterns suggest where to look for answers.\u003C/p>\n\u003Ch2 id=\"a-tool-for-reflection-not-judgment\">A tool for reflection, not judgment\u003C/h2>\n\u003Cp>The danger is obvious: these tools can be used to surveil, to categorize, to reduce people to profiles. I don’t want to build that.\u003C/p>\n\u003Cp>What I’m trying to build is a mirror. One that shows you patterns in your own life, patterns you might not notice because you’re too close to see them. Not to tell you what’s wrong, but to prompt the question: \u003Cem>is this what I intended?\u003C/em>\u003C/p>\n\u003Cp>The photo library on your phone is an accidental diary. Years of moments, accumulating. Most of us never read it. We scroll past, looking for a specific memory, never asking what the collection as a whole might reveal.\u003C/p>\n\u003Ch2 id=\"questions-worth-asking\">Questions worth asking\u003C/h2>\n\u003Cp>If you could see the aggregate truth of your photos:\u003C/p>\n\u003Cul>\n\u003Cli>Who do you photograph most? Is that who you want to remember?\u003C/li>\n\u003Cli>Where do your photos cluster? Is that where your life is, or where you wish it was?\u003C/li>\n\u003Cli>When do you take photos? What prompts you to capture a moment?\u003C/li>\n\u003Cli>How do your expressions change across contexts? With whom do you seem most at ease?\u003C/li>\n\u003C/ul>\n\u003Cp>These aren’t questions a machine can answer. But a machine can surface the data that makes the questions possible.\u003C/p>\n\u003Ch2 id=\"the-practice\">The practice\u003C/h2>\n\u003Cp>I don’t think this kind of reflection should be automated. No app should tell you “you seem happier on weekends” or “you don’t photograph your family enough.” That crosses from observation into judgment.\u003C/p>\n\u003Cp>Instead, the practice might be simpler: periodically look at what the signals say. Sit with the patterns. Notice what surprises you. Ask yourself why.\u003C/p>\n\u003Cp>The photo knows when and where. The face detection knows expressions and positions. Only you know what it meant.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>This thinking led to the \u003Ca href=\"/analyze\">image analysis tool\u003C/a> on this site—an experiment in surfacing signals without interpreting them. Upload a photo, see what’s extractable. The meaning remains yours to find.\u003C/em>\u003C/p>",{"headings":213,"localImagePaths":232,"remoteImagePaths":233,"frontmatter":234,"imagePaths":236},[214,217,220,223,226,229],{"depth":25,"slug":215,"text":216},"what-a-single-photo-contains","What a single photo contains",{"depth":25,"slug":218,"text":219},"the-limits-of-extraction","The limits of extraction",{"depth":25,"slug":221,"text":222},"where-self-knowledge-might-live","Where self-knowledge might live",{"depth":25,"slug":224,"text":225},"a-tool-for-reflection-not-judgment","A tool for reflection, not judgment",{"depth":25,"slug":227,"text":228},"questions-worth-asking","Questions worth asking",{"depth":25,"slug":230,"text":231},"the-practice","The practice",[],[],{"title":204,"date":235,"description":206},["Date","2025-01-30T00:00:00.000Z"],[],"on-what-photos-remember.md","on-guilt-and-regret",{"id":238,"data":240,"body":244,"filePath":245,"digest":246,"rendered":247,"legacyId":278},{"title":241,"date":242,"description":243},"On Guilt and Regret",["Date","2025-01-30T00:00:00.000Z"],"Why machines cannot see what we hide even from ourselves","Of all the emotions that shape human behavior, guilt and regret may be the most invisible. Not because they're rare—they're nearly universal—but because they live beneath the surface, often undetectable even to those who know us best.\n\nThis poses a fundamental problem for any system that claims to understand human emotion through observation.\n\n## The invisibility of internal states\n\nA camera can capture a face. An algorithm can measure the distance between eyebrows, the curve of lips, the direction of gaze. From these geometric facts, models attempt to infer emotional states: happy, sad, angry, surprised.\n\nBut guilt? Regret? These don't announce themselves on the face.\n\nConsider: a person sits alone, expression neutral, perhaps slightly downcast. They might be tired. They might be bored. They might be replaying a conversation from twenty years ago where they said something cruel to someone who loved them—someone now gone, the apology forever undelivered.\n\nThe face looks the same in all three cases. The machine sees pixels. The weight of unspoken remorse is invisible.\n\n## Why expression fails\n\nGuilt and regret are retrospective emotions. They require:\n\n- **Memory** of a past action or inaction\n- **Moral judgment** that the action was wrong\n- **Counterfactual thinking**—imagining how things could have been different\n- **Persistence**—the feeling doesn't resolve when the moment passes\n\nNone of these leave reliable physical traces. Unlike fear (elevated heart rate, widened eyes) or joy (genuine smiles engage the orbicularis oculi), guilt has no signature expression. Paul Ekman, who spent decades cataloging universal facial expressions, found no distinct \"guilt face.\" The closest markers—gaze aversion, slumped posture—are shared with shame, sadness, fatigue, and simple introversion.\n\nDarwin himself noted in *The Expression of the Emotions in Man and Animals* (1872) that complex social emotions like guilt show enormous cultural and individual variation. What looks like guilt in one person might be invisible in another.\n\n## The confession requirement\n\nHere's the uncomfortable truth: we typically only know someone feels guilt when they tell us.\n\nThis is why religious traditions developed confession. Why therapy works through dialogue. Why truth and reconciliation processes rely on testimony. The internal experience of guilt cannot be observed—it must be disclosed.\n\nA machine learning model trained on millions of faces will never learn to detect guilt reliably, because guilt doesn't present reliably. The training data itself is sparse: we rarely have ground truth labels for \"this person is experiencing guilt right now.\" We only have moments where someone admitted to guilt, and even then, we can't know if their face reflected it.\n\n## Stories of hidden weight\n\n**The parent who didn't say goodbye.** A father, rushing to work, brushes past his daughter's request to play. \"Later,\" he says. There is no later—an accident, a loss, a lifetime of replaying that moment. His face in photographs afterward shows nothing unusual. The guilt is entirely internal, carried silently for decades.\n\n**The survivor's burden.** In *Man's Search for Meaning*, Viktor Frankl describes Holocaust survivors who felt guilt for living when others died. This \"survivor's guilt\" manifests not as a detectable expression but as a persistent, quiet torment. Many survivors appeared functional, even successful. The weight was invisible.\n\n**The undone kindness.** A woman passes a homeless person every day for years. She never stops. One winter, the person is gone—frozen, she later learns. Her face shows nothing. But she changes her route, unable to pass that spot. The guilt reshapes her behavior, not her expression.\n\n## What machines actually see\n\nWhen AI systems claim to detect \"guilt\" or \"remorse,\" they're typically detecting:\n\n- **Downcast gaze** (also indicates sadness, shame, submission, or thought)\n- **Reduced facial animation** (also indicates depression, fatigue, or concentration)\n- **Self-touching gestures** (also indicates anxiety, discomfort, or habit)\n- **Speech patterns** like hedging or qualification (also indicates uncertainty or politeness)\n\nThese are correlates at best, not signatures. A skilled deceiver can fake them. A stoic person can feel crushing guilt while showing none of them.\n\nThe famous \"guilty look\" that dog owners swear they see? Research by Alexandra Horowitz at Barnard College showed dogs display \"guilty\" body language based on owner behavior, not their own actions. The dog who didn't eat the treat shows \"guilt\" if the owner acts accusatory. The dog who did eat it shows nothing if the owner acts normal.\n\nIf we can't even reliably detect guilt in dogs—creatures we've co-evolved with for 15,000 years—what hope do we have with algorithms?\n\n## The ethical boundary\n\nThis limitation isn't a bug to be fixed. It's a feature to be respected.\n\nIf machines could detect guilt, the implications would be dystopian. Imagine:\n\n- Job interviews where an algorithm scans for \"hidden guilt\"\n- Border crossings where your face is analyzed for \"signs of wrongdoing\"\n- Insurance claims denied because you \"looked guilty\"\n- Criminal justice systems that claim to detect remorse\n\nThe unreliability of guilt detection is a protection. It preserves the privacy of our inner moral lives. It keeps the space between action and conscience sacred.\n\n## What remains\n\nSo what can observation-based systems honestly do?\n\nThey can notice patterns without interpreting cause:\n- This person's demeanor changed after this date\n- This expression appears in contexts involving this topic\n- These behavioral markers cluster together\n\nThey can surface data for human interpretation:\n- \"Your facial expressions in photos from 2019 differ from 2020\"\n- \"You appear more animated when discussing X than Y\"\n\nBut they cannot—and should not—claim to know why.\n\nThe gap between observable signal and internal experience is not a limitation to overcome. It's a boundary to honor. Guilt and regret belong to the person who feels them. They are disclosed, not detected.\n\nSome things are meant to be told, not seen.\n\n---\n\n*This reflection shapes how I think about the [image analysis tools](/analyze) on this site. They report geometry—face positions, expression intensities, pose configurations. They deliberately refuse to interpret emotions like guilt, regret, or remorse. That's not a limitation of the technology. It's a design choice about what machines should claim to know.*\n\n## References\n\n- Darwin, C. (1872). *The Expression of the Emotions in Man and Animals*\n- Ekman, P. (2003). *Emotions Revealed: Recognizing Faces and Feelings*\n- Frankl, V. (1946). *Man's Search for Meaning*\n- Horowitz, A. (2009). \"Disambiguating the 'guilty look': Salient prompts to a familiar dog behaviour.\" *Behavioural Processes*, 81(3), 447-452.\n- Barrett, L.F. (2017). *How Emotions Are Made: The Secret Life of the Brain*","src/content/journal/on-guilt-and-regret.md","6821fd036531a2d6",{"html":248,"metadata":249},"\u003Cp>Of all the emotions that shape human behavior, guilt and regret may be the most invisible. Not because they’re rare—they’re nearly universal—but because they live beneath the surface, often undetectable even to those who know us best.\u003C/p>\n\u003Cp>This poses a fundamental problem for any system that claims to understand human emotion through observation.\u003C/p>\n\u003Ch2 id=\"the-invisibility-of-internal-states\">The invisibility of internal states\u003C/h2>\n\u003Cp>A camera can capture a face. An algorithm can measure the distance between eyebrows, the curve of lips, the direction of gaze. From these geometric facts, models attempt to infer emotional states: happy, sad, angry, surprised.\u003C/p>\n\u003Cp>But guilt? Regret? These don’t announce themselves on the face.\u003C/p>\n\u003Cp>Consider: a person sits alone, expression neutral, perhaps slightly downcast. They might be tired. They might be bored. They might be replaying a conversation from twenty years ago where they said something cruel to someone who loved them—someone now gone, the apology forever undelivered.\u003C/p>\n\u003Cp>The face looks the same in all three cases. The machine sees pixels. The weight of unspoken remorse is invisible.\u003C/p>\n\u003Ch2 id=\"why-expression-fails\">Why expression fails\u003C/h2>\n\u003Cp>Guilt and regret are retrospective emotions. They require:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Memory\u003C/strong> of a past action or inaction\u003C/li>\n\u003Cli>\u003Cstrong>Moral judgment\u003C/strong> that the action was wrong\u003C/li>\n\u003Cli>\u003Cstrong>Counterfactual thinking\u003C/strong>—imagining how things could have been different\u003C/li>\n\u003Cli>\u003Cstrong>Persistence\u003C/strong>—the feeling doesn’t resolve when the moment passes\u003C/li>\n\u003C/ul>\n\u003Cp>None of these leave reliable physical traces. Unlike fear (elevated heart rate, widened eyes) or joy (genuine smiles engage the orbicularis oculi), guilt has no signature expression. Paul Ekman, who spent decades cataloging universal facial expressions, found no distinct “guilt face.” The closest markers—gaze aversion, slumped posture—are shared with shame, sadness, fatigue, and simple introversion.\u003C/p>\n\u003Cp>Darwin himself noted in \u003Cem>The Expression of the Emotions in Man and Animals\u003C/em> (1872) that complex social emotions like guilt show enormous cultural and individual variation. What looks like guilt in one person might be invisible in another.\u003C/p>\n\u003Ch2 id=\"the-confession-requirement\">The confession requirement\u003C/h2>\n\u003Cp>Here’s the uncomfortable truth: we typically only know someone feels guilt when they tell us.\u003C/p>\n\u003Cp>This is why religious traditions developed confession. Why therapy works through dialogue. Why truth and reconciliation processes rely on testimony. The internal experience of guilt cannot be observed—it must be disclosed.\u003C/p>\n\u003Cp>A machine learning model trained on millions of faces will never learn to detect guilt reliably, because guilt doesn’t present reliably. The training data itself is sparse: we rarely have ground truth labels for “this person is experiencing guilt right now.” We only have moments where someone admitted to guilt, and even then, we can’t know if their face reflected it.\u003C/p>\n\u003Ch2 id=\"stories-of-hidden-weight\">Stories of hidden weight\u003C/h2>\n\u003Cp>\u003Cstrong>The parent who didn’t say goodbye.\u003C/strong> A father, rushing to work, brushes past his daughter’s request to play. “Later,” he says. There is no later—an accident, a loss, a lifetime of replaying that moment. His face in photographs afterward shows nothing unusual. The guilt is entirely internal, carried silently for decades.\u003C/p>\n\u003Cp>\u003Cstrong>The survivor’s burden.\u003C/strong> In \u003Cem>Man’s Search for Meaning\u003C/em>, Viktor Frankl describes Holocaust survivors who felt guilt for living when others died. This “survivor’s guilt” manifests not as a detectable expression but as a persistent, quiet torment. Many survivors appeared functional, even successful. The weight was invisible.\u003C/p>\n\u003Cp>\u003Cstrong>The undone kindness.\u003C/strong> A woman passes a homeless person every day for years. She never stops. One winter, the person is gone—frozen, she later learns. Her face shows nothing. But she changes her route, unable to pass that spot. The guilt reshapes her behavior, not her expression.\u003C/p>\n\u003Ch2 id=\"what-machines-actually-see\">What machines actually see\u003C/h2>\n\u003Cp>When AI systems claim to detect “guilt” or “remorse,” they’re typically detecting:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Downcast gaze\u003C/strong> (also indicates sadness, shame, submission, or thought)\u003C/li>\n\u003Cli>\u003Cstrong>Reduced facial animation\u003C/strong> (also indicates depression, fatigue, or concentration)\u003C/li>\n\u003Cli>\u003Cstrong>Self-touching gestures\u003C/strong> (also indicates anxiety, discomfort, or habit)\u003C/li>\n\u003Cli>\u003Cstrong>Speech patterns\u003C/strong> like hedging or qualification (also indicates uncertainty or politeness)\u003C/li>\n\u003C/ul>\n\u003Cp>These are correlates at best, not signatures. A skilled deceiver can fake them. A stoic person can feel crushing guilt while showing none of them.\u003C/p>\n\u003Cp>The famous “guilty look” that dog owners swear they see? Research by Alexandra Horowitz at Barnard College showed dogs display “guilty” body language based on owner behavior, not their own actions. The dog who didn’t eat the treat shows “guilt” if the owner acts accusatory. The dog who did eat it shows nothing if the owner acts normal.\u003C/p>\n\u003Cp>If we can’t even reliably detect guilt in dogs—creatures we’ve co-evolved with for 15,000 years—what hope do we have with algorithms?\u003C/p>\n\u003Ch2 id=\"the-ethical-boundary\">The ethical boundary\u003C/h2>\n\u003Cp>This limitation isn’t a bug to be fixed. It’s a feature to be respected.\u003C/p>\n\u003Cp>If machines could detect guilt, the implications would be dystopian. Imagine:\u003C/p>\n\u003Cul>\n\u003Cli>Job interviews where an algorithm scans for “hidden guilt”\u003C/li>\n\u003Cli>Border crossings where your face is analyzed for “signs of wrongdoing”\u003C/li>\n\u003Cli>Insurance claims denied because you “looked guilty”\u003C/li>\n\u003Cli>Criminal justice systems that claim to detect remorse\u003C/li>\n\u003C/ul>\n\u003Cp>The unreliability of guilt detection is a protection. It preserves the privacy of our inner moral lives. It keeps the space between action and conscience sacred.\u003C/p>\n\u003Ch2 id=\"what-remains\">What remains\u003C/h2>\n\u003Cp>So what can observation-based systems honestly do?\u003C/p>\n\u003Cp>They can notice patterns without interpreting cause:\u003C/p>\n\u003Cul>\n\u003Cli>This person’s demeanor changed after this date\u003C/li>\n\u003Cli>This expression appears in contexts involving this topic\u003C/li>\n\u003Cli>These behavioral markers cluster together\u003C/li>\n\u003C/ul>\n\u003Cp>They can surface data for human interpretation:\u003C/p>\n\u003Cul>\n\u003Cli>“Your facial expressions in photos from 2019 differ from 2020”\u003C/li>\n\u003Cli>“You appear more animated when discussing X than Y”\u003C/li>\n\u003C/ul>\n\u003Cp>But they cannot—and should not—claim to know why.\u003C/p>\n\u003Cp>The gap between observable signal and internal experience is not a limitation to overcome. It’s a boundary to honor. Guilt and regret belong to the person who feels them. They are disclosed, not detected.\u003C/p>\n\u003Cp>Some things are meant to be told, not seen.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>This reflection shapes how I think about the \u003Ca href=\"/analyze\">image analysis tools\u003C/a> on this site. They report geometry—face positions, expression intensities, pose configurations. They deliberately refuse to interpret emotions like guilt, regret, or remorse. That’s not a limitation of the technology. It’s a design choice about what machines should claim to know.\u003C/em>\u003C/p>\n\u003Ch2 id=\"references\">References\u003C/h2>\n\u003Cul>\n\u003Cli>Darwin, C. (1872). \u003Cem>The Expression of the Emotions in Man and Animals\u003C/em>\u003C/li>\n\u003Cli>Ekman, P. (2003). \u003Cem>Emotions Revealed: Recognizing Faces and Feelings\u003C/em>\u003C/li>\n\u003Cli>Frankl, V. (1946). \u003Cem>Man’s Search for Meaning\u003C/em>\u003C/li>\n\u003Cli>Horowitz, A. (2009). “Disambiguating the ‘guilty look’: Salient prompts to a familiar dog behaviour.” \u003Cem>Behavioural Processes\u003C/em>, 81(3), 447-452.\u003C/li>\n\u003Cli>Barrett, L.F. (2017). \u003Cem>How Emotions Are Made: The Secret Life of the Brain\u003C/em>\u003C/li>\n\u003C/ul>",{"headings":250,"localImagePaths":273,"remoteImagePaths":274,"frontmatter":275,"imagePaths":277},[251,254,257,260,263,266,269,272],{"depth":25,"slug":252,"text":253},"the-invisibility-of-internal-states","The invisibility of internal states",{"depth":25,"slug":255,"text":256},"why-expression-fails","Why expression fails",{"depth":25,"slug":258,"text":259},"the-confession-requirement","The confession requirement",{"depth":25,"slug":261,"text":262},"stories-of-hidden-weight","Stories of hidden weight",{"depth":25,"slug":264,"text":265},"what-machines-actually-see","What machines actually see",{"depth":25,"slug":267,"text":268},"the-ethical-boundary","The ethical boundary",{"depth":25,"slug":270,"text":271},"what-remains","What remains",{"depth":25,"slug":131,"text":132},[],[],{"title":241,"date":276,"description":243},["Date","2025-01-30T00:00:00.000Z"],[],"on-guilt-and-regret.md","on-souls",{"id":279,"data":281,"body":285,"filePath":286,"digest":287,"rendered":288,"legacyId":321},{"title":282,"date":283,"description":284},"On Souls",["Date","2025-02-01T00:00:00.000Z"],"What remains when we subtract everything observable?","I've spent months building tools that extract signals from images—faces detected, expressions measured, poses mapped. The technology works. It finds what it looks for. And yet, the more precisely I can measure the visible, the more I'm haunted by what remains invisible.\n\nWhat is this thing we call the soul?\n\n## The subtraction problem\n\nIf I photograph you, I can extract:\n- The geometry of your face (478 landmarks, if using MediaPipe)\n- Your expression (52 blendshapes measuring every muscle movement)\n- Your pose (33 body keypoints in three-dimensional space)\n- The scene around you (indoor, outdoor, day, night)\n- The metadata (when, where, what device)\n\nI can aggregate these across thousands of images and find patterns. I can track how your expressions cluster, how your posture shifts across contexts, who appears beside you and how often.\n\nBut if I subtract all of this—every measurable signal—something remains. The part that decides what the smile means. The part that knows what you were thinking when the photo was taken. The observer behind the observation.\n\nIs that the soul?\n\n## What the ancients said\n\nThe concept appears everywhere, wearing different names:\n\n- The Hebrew *nephesh*—the breath that animates\n- The Greek *psyche*—that which flutters away at death\n- The Sanskrit *atman*—the self that persists\n- The Chinese *hun* and *po*—the ethereal and corporeal souls, plural\n\nAristotle thought the soul was the \"form\" of the body—not a separate thing but the organizing principle that makes matter alive. Descartes split us in two: extended substance (the body) and thinking substance (the mind/soul). The Buddhists took another path entirely: *anatta*, no-self, the idea that what we call soul is an illusion, a story we tell about a process.\n\nWhat strikes me is how every tradition grapples with the same observation: there's the body we can see, and there's... something else. Something that experiences.\n\n## The question of multiplicity\n\nThis brings us to a strange case: dissociative identity disorder, once called multiple personality disorder.\n\nIn DID, a single body hosts what appear to be distinct identities—different names, different memories, different mannerisms, sometimes different handedness or allergies. Brain imaging studies show different patterns of activation for different alters. The body is one; the selves are many.\n\nIf we believe in souls, this poses a genuine puzzle: does such a person have multiple souls?\n\n**The theological answers vary:**\n\nSome religious traditions would say no—one body, one soul, and the apparent multiplicity is a disorder of the mind, not a fracturing of the essential self. The soul remains singular even if the personality fragments.\n\nOthers might say the concept doesn't apply—that DID is precisely the kind of case that reveals the soul as metaphor rather than substance. There is no ghost in the machine; there is only the machine, running different programs.\n\n**The clinical perspective:**\n\nPsychiatry treats DID as a response to severe trauma, usually in early childhood. The mind, overwhelmed, partitions itself. The \"alters\" are understood as dissociated parts of a single consciousness, not separate beings. Treatment often aims at integration—helping the parts recognize themselves as aspects of one whole.\n\nBut here's what's interesting: many people with DID report that their alters experience themselves as genuinely distinct. They have their own preferences, their own histories, their own sense of being. From the inside, multiplicity feels real.\n\nWho are we to say it isn't?\n\n## What signals cannot capture\n\nI built an image analysis tool that reports what it sees without interpretation. It will tell you that a face shows \"mouth corners raised\" but refuses to conclude \"happy.\" It measures head angle but won't infer intent. This was a deliberate choice—humility encoded in software.\n\nBut watching this tool work has taught me something: even if we could measure everything, we wouldn't capture the soul.\n\nNot because the soul is supernatural (it might or might not be), but because the soul—whatever it is—is the experiencer, not the experienced. It's the subject, not the object. Every measurement we make is of the body, the face, the behavior. The thing that witnesses these measurements remains outside the frame.\n\nWilliam James wrote about this in *The Principles of Psychology* (1890). He called it the \"I\" versus the \"Me\"—the self that observes versus the self that is observed. The Me can be studied: your body, your memories, your social roles. The I cannot. It's always the one doing the studying.\n\n## The multiplicity in all of us\n\nPerhaps DID is an extreme case of something universal.\n\nDon't we all contain multitudes? The self I am at work differs from the self I am with my family. The me who writes these words is not quite the me who will read them tomorrow. We speak of \"being of two minds\" about a decision. We surprise ourselves—who *was* that, we wonder, who said that cruel thing, who acted so bravely, who made that choice?\n\nWalt Whitman wrote: \"I am large, I contain multitudes.\"\n\nMaybe the soul isn't a single point but a process. Not a thing but a happening. And maybe what we see in DID isn't a soul shattered but a soul whose multiplicity became visible.\n\n## The limits of detection\n\nI keep returning to this: some things can only be known from the inside.\n\nGuilt and regret, as I wrote recently, leave no reliable trace on the face. They must be confessed, not detected. The same is true for the soul. No camera will capture it. No algorithm will extract it. Not because our technology is insufficient, but because the soul—by definition—is not an object to be observed.\n\nIt is the observer.\n\nThis isn't mysticism. It's epistemology. There are things about human experience that are first-person by nature. Pain, consciousness, the felt sense of being. These are real (more real to us than anything), yet they resist third-person capture.\n\n## An unanswerable question\n\nDoes a person with DID have multiple souls?\n\nI don't know. I suspect the question itself might be malformed—that \"soul\" is a word we use for something we don't understand, and asking how many souls are present is like asking what color a song is.\n\nBut I find the question valuable precisely because it reveals the limits of our categories. It shows us where our tools break down, where measurement gives way to mystery.\n\nThe body can be photographed. The face can be analyzed. The behavior can be catalogued. But the one who lives behind the eyes—singular or plural, permanent or passing—remains beyond the reach of any instrument I can build.\n\nAnd perhaps that's as it should be.\n\n---\n\n*This is part of an ongoing exploration of what signals can and cannot tell us about human nature. The [image analysis tool](/analyze) on this site is an experiment in honest observation—capturing geometry without claiming to capture the soul.*\n\n## Further reading\n\n- James, W. (1890). *The Principles of Psychology*, Chapter X: The Consciousness of Self\n- van der Kolk, B. (2014). *The Body Keeps the Score* (on trauma and dissociation)\n- Sacks, O. (1985). *The Man Who Mistook His Wife for a Hat* (case studies in identity)\n- Parfit, D. (1984). *Reasons and Persons* (philosophical exploration of personal identity)\n- Dennett, D. (1991). *Consciousness Explained* (materialist view of self)","src/content/journal/on-souls.md","5fc1caf041d1bfee",{"html":289,"metadata":290},"\u003Cp>I’ve spent months building tools that extract signals from images—faces detected, expressions measured, poses mapped. The technology works. It finds what it looks for. And yet, the more precisely I can measure the visible, the more I’m haunted by what remains invisible.\u003C/p>\n\u003Cp>What is this thing we call the soul?\u003C/p>\n\u003Ch2 id=\"the-subtraction-problem\">The subtraction problem\u003C/h2>\n\u003Cp>If I photograph you, I can extract:\u003C/p>\n\u003Cul>\n\u003Cli>The geometry of your face (478 landmarks, if using MediaPipe)\u003C/li>\n\u003Cli>Your expression (52 blendshapes measuring every muscle movement)\u003C/li>\n\u003Cli>Your pose (33 body keypoints in three-dimensional space)\u003C/li>\n\u003Cli>The scene around you (indoor, outdoor, day, night)\u003C/li>\n\u003Cli>The metadata (when, where, what device)\u003C/li>\n\u003C/ul>\n\u003Cp>I can aggregate these across thousands of images and find patterns. I can track how your expressions cluster, how your posture shifts across contexts, who appears beside you and how often.\u003C/p>\n\u003Cp>But if I subtract all of this—every measurable signal—something remains. The part that decides what the smile means. The part that knows what you were thinking when the photo was taken. The observer behind the observation.\u003C/p>\n\u003Cp>Is that the soul?\u003C/p>\n\u003Ch2 id=\"what-the-ancients-said\">What the ancients said\u003C/h2>\n\u003Cp>The concept appears everywhere, wearing different names:\u003C/p>\n\u003Cul>\n\u003Cli>The Hebrew \u003Cem>nephesh\u003C/em>—the breath that animates\u003C/li>\n\u003Cli>The Greek \u003Cem>psyche\u003C/em>—that which flutters away at death\u003C/li>\n\u003Cli>The Sanskrit \u003Cem>atman\u003C/em>—the self that persists\u003C/li>\n\u003Cli>The Chinese \u003Cem>hun\u003C/em> and \u003Cem>po\u003C/em>—the ethereal and corporeal souls, plural\u003C/li>\n\u003C/ul>\n\u003Cp>Aristotle thought the soul was the “form” of the body—not a separate thing but the organizing principle that makes matter alive. Descartes split us in two: extended substance (the body) and thinking substance (the mind/soul). The Buddhists took another path entirely: \u003Cem>anatta\u003C/em>, no-self, the idea that what we call soul is an illusion, a story we tell about a process.\u003C/p>\n\u003Cp>What strikes me is how every tradition grapples with the same observation: there’s the body we can see, and there’s… something else. Something that experiences.\u003C/p>\n\u003Ch2 id=\"the-question-of-multiplicity\">The question of multiplicity\u003C/h2>\n\u003Cp>This brings us to a strange case: dissociative identity disorder, once called multiple personality disorder.\u003C/p>\n\u003Cp>In DID, a single body hosts what appear to be distinct identities—different names, different memories, different mannerisms, sometimes different handedness or allergies. Brain imaging studies show different patterns of activation for different alters. The body is one; the selves are many.\u003C/p>\n\u003Cp>If we believe in souls, this poses a genuine puzzle: does such a person have multiple souls?\u003C/p>\n\u003Cp>\u003Cstrong>The theological answers vary:\u003C/strong>\u003C/p>\n\u003Cp>Some religious traditions would say no—one body, one soul, and the apparent multiplicity is a disorder of the mind, not a fracturing of the essential self. The soul remains singular even if the personality fragments.\u003C/p>\n\u003Cp>Others might say the concept doesn’t apply—that DID is precisely the kind of case that reveals the soul as metaphor rather than substance. There is no ghost in the machine; there is only the machine, running different programs.\u003C/p>\n\u003Cp>\u003Cstrong>The clinical perspective:\u003C/strong>\u003C/p>\n\u003Cp>Psychiatry treats DID as a response to severe trauma, usually in early childhood. The mind, overwhelmed, partitions itself. The “alters” are understood as dissociated parts of a single consciousness, not separate beings. Treatment often aims at integration—helping the parts recognize themselves as aspects of one whole.\u003C/p>\n\u003Cp>But here’s what’s interesting: many people with DID report that their alters experience themselves as genuinely distinct. They have their own preferences, their own histories, their own sense of being. From the inside, multiplicity feels real.\u003C/p>\n\u003Cp>Who are we to say it isn’t?\u003C/p>\n\u003Ch2 id=\"what-signals-cannot-capture\">What signals cannot capture\u003C/h2>\n\u003Cp>I built an image analysis tool that reports what it sees without interpretation. It will tell you that a face shows “mouth corners raised” but refuses to conclude “happy.” It measures head angle but won’t infer intent. This was a deliberate choice—humility encoded in software.\u003C/p>\n\u003Cp>But watching this tool work has taught me something: even if we could measure everything, we wouldn’t capture the soul.\u003C/p>\n\u003Cp>Not because the soul is supernatural (it might or might not be), but because the soul—whatever it is—is the experiencer, not the experienced. It’s the subject, not the object. Every measurement we make is of the body, the face, the behavior. The thing that witnesses these measurements remains outside the frame.\u003C/p>\n\u003Cp>William James wrote about this in \u003Cem>The Principles of Psychology\u003C/em> (1890). He called it the “I” versus the “Me”—the self that observes versus the self that is observed. The Me can be studied: your body, your memories, your social roles. The I cannot. It’s always the one doing the studying.\u003C/p>\n\u003Ch2 id=\"the-multiplicity-in-all-of-us\">The multiplicity in all of us\u003C/h2>\n\u003Cp>Perhaps DID is an extreme case of something universal.\u003C/p>\n\u003Cp>Don’t we all contain multitudes? The self I am at work differs from the self I am with my family. The me who writes these words is not quite the me who will read them tomorrow. We speak of “being of two minds” about a decision. We surprise ourselves—who \u003Cem>was\u003C/em> that, we wonder, who said that cruel thing, who acted so bravely, who made that choice?\u003C/p>\n\u003Cp>Walt Whitman wrote: “I am large, I contain multitudes.”\u003C/p>\n\u003Cp>Maybe the soul isn’t a single point but a process. Not a thing but a happening. And maybe what we see in DID isn’t a soul shattered but a soul whose multiplicity became visible.\u003C/p>\n\u003Ch2 id=\"the-limits-of-detection\">The limits of detection\u003C/h2>\n\u003Cp>I keep returning to this: some things can only be known from the inside.\u003C/p>\n\u003Cp>Guilt and regret, as I wrote recently, leave no reliable trace on the face. They must be confessed, not detected. The same is true for the soul. No camera will capture it. No algorithm will extract it. Not because our technology is insufficient, but because the soul—by definition—is not an object to be observed.\u003C/p>\n\u003Cp>It is the observer.\u003C/p>\n\u003Cp>This isn’t mysticism. It’s epistemology. There are things about human experience that are first-person by nature. Pain, consciousness, the felt sense of being. These are real (more real to us than anything), yet they resist third-person capture.\u003C/p>\n\u003Ch2 id=\"an-unanswerable-question\">An unanswerable question\u003C/h2>\n\u003Cp>Does a person with DID have multiple souls?\u003C/p>\n\u003Cp>I don’t know. I suspect the question itself might be malformed—that “soul” is a word we use for something we don’t understand, and asking how many souls are present is like asking what color a song is.\u003C/p>\n\u003Cp>But I find the question valuable precisely because it reveals the limits of our categories. It shows us where our tools break down, where measurement gives way to mystery.\u003C/p>\n\u003Cp>The body can be photographed. The face can be analyzed. The behavior can be catalogued. But the one who lives behind the eyes—singular or plural, permanent or passing—remains beyond the reach of any instrument I can build.\u003C/p>\n\u003Cp>And perhaps that’s as it should be.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>This is part of an ongoing exploration of what signals can and cannot tell us about human nature. The \u003Ca href=\"/analyze\">image analysis tool\u003C/a> on this site is an experiment in honest observation—capturing geometry without claiming to capture the soul.\u003C/em>\u003C/p>\n\u003Ch2 id=\"further-reading\">Further reading\u003C/h2>\n\u003Cul>\n\u003Cli>James, W. (1890). \u003Cem>The Principles of Psychology\u003C/em>, Chapter X: The Consciousness of Self\u003C/li>\n\u003Cli>van der Kolk, B. (2014). \u003Cem>The Body Keeps the Score\u003C/em> (on trauma and dissociation)\u003C/li>\n\u003Cli>Sacks, O. (1985). \u003Cem>The Man Who Mistook His Wife for a Hat\u003C/em> (case studies in identity)\u003C/li>\n\u003Cli>Parfit, D. (1984). \u003Cem>Reasons and Persons\u003C/em> (philosophical exploration of personal identity)\u003C/li>\n\u003Cli>Dennett, D. (1991). \u003Cem>Consciousness Explained\u003C/em> (materialist view of self)\u003C/li>\n\u003C/ul>",{"headings":291,"localImagePaths":316,"remoteImagePaths":317,"frontmatter":318,"imagePaths":320},[292,295,298,301,304,307,310,313],{"depth":25,"slug":293,"text":294},"the-subtraction-problem","The subtraction problem",{"depth":25,"slug":296,"text":297},"what-the-ancients-said","What the ancients said",{"depth":25,"slug":299,"text":300},"the-question-of-multiplicity","The question of multiplicity",{"depth":25,"slug":302,"text":303},"what-signals-cannot-capture","What signals cannot capture",{"depth":25,"slug":305,"text":306},"the-multiplicity-in-all-of-us","The multiplicity in all of us",{"depth":25,"slug":308,"text":309},"the-limits-of-detection","The limits of detection",{"depth":25,"slug":311,"text":312},"an-unanswerable-question","An unanswerable question",{"depth":25,"slug":314,"text":315},"further-reading","Further reading",[],[],{"title":282,"date":319,"description":284},["Date","2025-02-01T00:00:00.000Z"],[],"on-souls.md"]